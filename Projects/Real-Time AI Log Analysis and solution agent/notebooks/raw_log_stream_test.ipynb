{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15d85c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured to stream from: ../data/raw_logs/hdfs_v2/HDFS_v2/node_logs/hadoop-hdfs-datanode-mesos-01.log\n",
      "\n",
      "Setup complete. Paths configured for Raw Log Streaming Test.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time # For simulating real-time delay\n",
    "import hashlib \n",
    "import pandas as pd # For saving templates and offset index\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for raw log input\n",
    "RAW_LOGS_DIR = \"../data/raw_logs/hdfs_v2/HDFS_v2/node_logs/\"\n",
    "# Specific file to stream for this test\n",
    "SINGLE_RAW_LOG_FILE_PATH = os.path.join(RAW_LOGS_DIR, \"hadoop-hdfs-datanode-mesos-01.log\") # <--- Your specified file\n",
    "\n",
    "# Output paths for generated/live-updated files\n",
    "# These will be written in APPEND mode to simulate continuous updates\n",
    "OUTPUT_TEMPLATES_CSV = \"../data/templates/hdfs_v2_templates_stream_test.csv\"\n",
    "OUTPUT_PARSED_JSONL = \"../data/parsed_logs/hdfs_v2_parsed_stream_test.jsonl\"\n",
    "OUTPUT_OFFSET_INDEX_FILE = \"../data/parsed_logs/hdfs_v2_offset_index_stream_test.csv\"\n",
    "\n",
    "# Simulate stream delay (seconds per raw log line)\n",
    "STREAM_DELAY_SECONDS = 0.001 # 1 millisecond delay per line (adjust as needed for speed)\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(os.path.dirname(OUTPUT_TEMPLATES_CSV), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(OUTPUT_PARSED_JSONL), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(OUTPUT_OFFSET_INDEX_FILE), exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Confirm input raw log file exists ---\n",
    "if not os.path.exists(SINGLE_RAW_LOG_FILE_PATH):\n",
    "    print(f\"ERROR: Specified raw log file NOT FOUND at {SINGLE_RAW_LOG_FILE_PATH}.\")\n",
    "    print(\"Please ensure your raw HDFS log files are correctly placed in that directory.\")\n",
    "    log_file_to_stream = None # Set to None to prevent errors\n",
    "else:\n",
    "    log_file_to_stream = SINGLE_RAW_LOG_FILE_PATH\n",
    "    print(f\"Configured to stream from: {log_file_to_stream}\")\n",
    "\n",
    "print(\"\\nSetup complete. Paths configured for Raw Log Streaming Test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4dfc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log parsing and normalization functions defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Core HDFS Log Header Regex ---\n",
    "HDFS_HEADER_REGEX = re.compile(\n",
    "    r'^(?P<Date>\\d{4}-\\d{2}-\\d{2})\\s'  \n",
    "    r'(?P<Time>\\d{2}:\\d{2}:\\d{2},\\d{3})\\s' \n",
    "    r'(?P<Level>[A-Z]+)\\s'                \n",
    "    r'(?P<Component>[\\w\\._-]+(?:\\[[\\w\\s\\.-]+\\])?):?\\s+' \n",
    "    r'(?P<Content>.*)'                    \n",
    ")\n",
    "\n",
    "# --- Define NORMALIZATION_RULES for Log Content ---\n",
    "NORMALIZATION_RULES = [\n",
    "    (re.compile(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}(:\\d+)?\\b'), '<IP_ADDRESS>'), \n",
    "    (re.compile(r'\\b(?:blk_|BP-|DS-)[-_a-zA-Z0-9]+\\b'), '<HDFS_ID>'), \n",
    "    (re.compile(r'\\b(?:mesos-master-\\d+|nodename \\d+@mesos-master-\\d+|localhost)\\b'), '<HOSTNAME_OR_NODE>'), \n",
    "    (re.compile(r'\\b([a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12})\\b'), '<UUID>'), \n",
    "    (re.compile(r'\\b(?:/usr/local/hadoop|/opt/hdfs/data|/[a-zA-Z0-9/\\._-]+/\\.jar)[\\w\\/\\.-]*\\b'), '<HADOOP_PATH>'), \n",
    "    (re.compile(r'[-_a-zA-Z0-9]+\\.jar'), '<JAR_FILE>'), \n",
    "    (re.compile(r'\\b(?:version|build)\\s*=\\s*[\\w\\.\\-]+(?:(?:-|:)?\\s*[\\w\\.:-]+)?(?:; compiled by [\\w\\s\\']+\\son\\s[\\d\\-T:]+Z)?\\b'), '<VERSION_INFO>'), \n",
    "    (re.compile(r'java = \\d+\\.\\d+\\.\\d+_?\\d*'), '<JAVA_VERSION>'), \n",
    "    (re.compile(r'took\\s+\\d+ms\\s+\\(threshold=\\d+ms\\)'), 'took <NUM>ms (threshold=<NUM>ms)'), \n",
    "    (re.compile(r'Scheduled snapshot period at \\d+ second\\(s\\)\\.'), 'Scheduled snapshot period at <NUM> second(s).'), \n",
    "    (re.compile(r'Balanced bandwith is \\d+ bytes/s'), 'Balancing bandwith is <NUM> bytes/s'), \n",
    "    (re.compile(r'Number threads for balancing is \\d+'), 'Number threads for balancing is <NUM>'), \n",
    "    (re.compile(r'registered UNIX signal handlers for \\[[\\w,\\s]+\\]'), 'registered UNIX signal handlers for [<SIGNAL_LIST>]'), \n",
    "    (re.compile(r'Listening HTTP traffic on /<IP_ADDRESS>:<NUM>'), 'Listening HTTP traffic on <IP_ADDRESS>:<NUM>'), \n",
    "    (re.compile(r'IPC server at /<IP_ADDRESS>:<NUM>'), 'IPC server at <IP_ADDRESS>:<NUM>'), \n",
    "    (re.compile(r'Jetty bound to port \\d+'), 'Jetty bound to port <NUM>'), \n",
    "    (re.compile(r'jetty-[\\d\\.]+'), 'jetty-<VERSION>'), \n",
    "    (re.compile(r'block report from <IP_ADDRESS>\\. Number of blocks: \\d+'), 'block report from <IP_ADDRESS>. Number of blocks: <NUM>'), \n",
    "    (re.compile(r'Logging to org\\.slf4j\\.impl\\.Log4jLoggerAdapter\\(org\\.mortbay\\.log\\) via org\\.mortbay\\.log\\.Slf4jLog'), 'Logging to <LOGGER_IMPL> via <LOGGER_BRIDGE>'), \n",
    "    (re.compile(r'Unable to initialize FileSignerSecretProvider, falling back to use random secrets\\.'), 'Unable to initialize FileSignerSecretProvider, falling back to use random secrets.'), \n",
    "    (re.compile(r'\\d+'), '<NUM>'), \n",
    "]\n",
    "\n",
    "def normalize_log_content(content):\n",
    "    \"\"\"Applies normalization rules to the log message content.\"\"\"\n",
    "    normalized_content = content\n",
    "    for regex, placeholder in NORMALIZATION_RULES:\n",
    "        normalized_content = regex.sub(placeholder, normalized_content)\n",
    "    # Further cleanup: remove extra spaces created by replacements\n",
    "    normalized_content = re.sub(r'\\s+', ' ', normalized_content).strip()\n",
    "    return normalized_content\n",
    "\n",
    "def parse_log_line_hybrid_single(raw_line):\n",
    "    \"\"\"\n",
    "    Parses a single HDFS log line for its header components.\n",
    "    Returns parsed dictionary or None if header does not match.\n",
    "    \"\"\"\n",
    "    match = HDFS_HEADER_REGEX.match(raw_line)\n",
    "    if not match:\n",
    "        return None \n",
    "    groups = match.groupdict()\n",
    "    return {\n",
    "        \"timestamp\": f\"{groups.get('Date', '')} {groups.get('Time', '')}\",\n",
    "        \"level\": groups.get('Level', ''),\n",
    "        \"component\": groups.get('Component', ''),\n",
    "        \"content_raw\": groups.get('Content', '').strip(),\n",
    "    }\n",
    "\n",
    "print(\"Log parsing and normalization functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e43a6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simulating Raw Log Stream & On-the-Fly Parsing/Indexing ---\n",
      "Streaming from: ../data/raw_logs/hdfs_v2/HDFS_v2/node_logs/hadoop-hdfs-datanode-mesos-01.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85c559d9aa745959c4c49e6edb81f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Streaming hadoop-hdfs-datanode-mesos-01.log: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(log_file_to_stream, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m, errors=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_raw:\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Use tqdm to show progress for reading the single log file\u001b[39;00m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, raw_line \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(f_raw), desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStreaming \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.path.basename(log_file_to_stream)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m         time.sleep(STREAM_DELAY_SECONDS) \u001b[38;5;66;03m# Simulate real-time delay\u001b[39;00m\n\u001b[32m     31\u001b[39m         \u001b[38;5;66;03m# Step 1: Attempt to parse the current raw line as a new header\u001b[39;00m\n\u001b[32m     32\u001b[39m         header_info = parse_log_line_hybrid_single(raw_line)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(f\"--- Simulating Raw Log Stream & On-the-Fly Parsing/Indexing ---\")\n",
    "\n",
    "# --- Global state for multi-line log assembly and offset map ---\n",
    "current_multi_line_log_buffer = [] # Buffer for lines of a single logical log entry\n",
    "last_parsed_multi_line_header_info = None # Stores header info for the current logical log entry\n",
    "parsed_line_counter = 0 # Counter for unique line IDs in the parsed JSONL file\n",
    "unique_templates_map = {} # To store unique templates discovered\n",
    "\n",
    "# --- Clear previous test files (if any) ---\n",
    "if os.path.exists(OUTPUT_PARSED_JSONL):\n",
    "    os.remove(OUTPUT_PARSED_JSONL)\n",
    "    print(f\"Cleared existing {OUTPUT_PARSED_JSONL}\")\n",
    "if os.path.exists(OUTPUT_OFFSET_INDEX_FILE):\n",
    "    os.remove(OUTPUT_OFFSET_INDEX_FILE)\n",
    "    print(f\"Cleared existing {OUTPUT_OFFSET_INDEX_FILE}\")\n",
    "if os.path.exists(OUTPUT_TEMPLATES_CSV):\n",
    "    os.remove(OUTPUT_TEMPLATES_CSV)\n",
    "    print(f\"Cleared existing {OUTPUT_TEMPLATES_CSV}\")\n",
    "\n",
    "# Open output files in APPEND mode for continuous writing\n",
    "try:\n",
    "    with open(OUTPUT_PARSED_JSONL, 'a', encoding='utf-8') as f_parsed_jsonl, \\\n",
    "         open(OUTPUT_OFFSET_INDEX_FILE, 'a', encoding='utf-8') as f_offset_index:\n",
    "        \n",
    "        print(f\"Streaming from: {log_file_to_stream}\")\n",
    "        with open(log_file_to_stream, 'r', encoding='utf-8', errors='ignore') as f_raw:\n",
    "            # Use tqdm to show progress for reading the single log file\n",
    "            for i, raw_line in tqdm(enumerate(f_raw), desc=f\"Streaming {os.path.basename(log_file_to_stream)}\"):\n",
    "                time.sleep(STREAM_DELAY_SECONDS) # Simulate real-time delay\n",
    "                \n",
    "                # Step 1: Attempt to parse the current raw line as a new header\n",
    "                header_info = parse_log_line_hybrid_single(raw_line)\n",
    "\n",
    "                if header_info: # This line starts a new logical log entry\n",
    "                    # Step 2: Process the previous multi-line log entry (if buffer has content)\n",
    "                    if current_multi_line_log_buffer and last_parsed_multi_line_header_info:\n",
    "                        full_original_message = \"\".join(current_multi_line_log_buffer).strip()\n",
    "                        \n",
    "                        # Get content for normalization and templating\n",
    "                        content_to_normalize = last_parsed_multi_line_header_info['content_raw'] \n",
    "                        normalized_template_key = normalize_log_content(content_to_normalize)\n",
    "                        \n",
    "                        # Discover unique template\n",
    "                        if normalized_template_key not in unique_templates_map:\n",
    "                            event_id_hash = hashlib.md5(normalized_template_key.encode('utf-8')).hexdigest()[:8].upper()\n",
    "                            unique_templates_map[normalized_template_key] = {\n",
    "                                'EventId': f'HDFS_{event_id_hash}', \n",
    "                                'EventTemplate': normalized_template_key, \n",
    "                                'Description': 'Auto-generated template (Needs human review)', \n",
    "                                'SampleOriginalMessage': full_original_message, \n",
    "                                'SampleLevel': last_parsed_multi_line_header_info['level'], \n",
    "                                'SampleComponent': last_parsed_multi_line_header_info['component'], \n",
    "                            }\n",
    "                        \n",
    "                        # Prepare the final parsed entry JSON\n",
    "                        final_parsed_entry = {\n",
    "                            \"line_id_in_file_header\": last_parsed_multi_line_header_info['line_id_in_file_header'], \n",
    "                            \"source_file\": last_parsed_multi_line_header_info['source_file'],\n",
    "                            \"original_log_full\": full_original_message, \n",
    "                            \"timestamp\": last_parsed_multi_line_header_info['timestamp'],\n",
    "                            \"level\": last_parsed_multi_line_header_info['level'],\n",
    "                            \"component\": last_parsed_multi_line_header_info['component'],\n",
    "                            \"event_id\": unique_templates_map[normalized_template_key]['EventId'], \n",
    "                            \"event_template\": unique_templates_map[normalized_template_key]['EventTemplate'],\n",
    "                            \"parameters\": \"\" \n",
    "                        }\n",
    "                        \n",
    "                        # Step 3: Append to Parsed JSONL and Byte-Offset Index\n",
    "                        json_line = json.dumps(final_parsed_entry) + '\\n'\n",
    "                        \n",
    "                        # Store current byte offset before writing this line\n",
    "                        current_byte_offset = f_parsed_jsonl.tell()\n",
    "                        \n",
    "                        # Write parsed JSONL entry\n",
    "                        f_parsed_jsonl.write(json_line)\n",
    "                        \n",
    "                        # Write byte-offset index entry\n",
    "                        offset_index_entry = {\n",
    "                            'source_file': final_parsed_entry['source_file'],\n",
    "                            'line_id_in_file_header': final_parsed_entry['line_id_in_file_header'],\n",
    "                            'byte_offset': current_byte_offset\n",
    "                        }\n",
    "                        f_offset_index.write(json.dumps(offset_index_entry) + '\\n')\n",
    "                        \n",
    "                        parsed_line_counter += 1 # Increment global counter for parsed lines\n",
    "\n",
    "                    # Step 4: Start new logical log entry\n",
    "                    current_multi_line_log_buffer = [raw_line]\n",
    "                    last_parsed_multi_line_header_info = header_info\n",
    "                    # Add current line_id and source_file for the new header\n",
    "                    last_parsed_multi_line_header_info['line_id_in_file_header'] = i + 1 \n",
    "                    last_parsed_multi_line_header_info['source_file'] = os.path.basename(log_file_to_stream)\n",
    "\n",
    "                else: # This line is a continuation\n",
    "                    current_multi_line_log_buffer.append(raw_line)\n",
    "                    # Handle cases where buffer might be empty at start (e.g., file doesn't start with header)\n",
    "                    if not last_parsed_multi_line_header_info and raw_line.strip():\n",
    "                        # These are lines that cannot be associated with a preceding header. Drop them.\n",
    "                        current_multi_line_log_buffer = [] \n",
    "\n",
    "        # --- FINAL FLUSH: Process the very last log entry after file finishes ---\n",
    "        if current_multi_line_log_buffer and last_parsed_multi_line_header_info:\n",
    "            full_original_message = \"\".join(current_multi_line_log_buffer).strip()\n",
    "            content_to_normalize = last_parsed_multi_line_header_info['content_raw']\n",
    "            normalized_template_key = normalize_log_content(content_to_normalize)\n",
    "            \n",
    "            if normalized_template_key not in unique_templates_map:\n",
    "                event_id_hash = hashlib.md5(normalized_template_key.encode('utf-8')).hexdigest()[:8].upper()\n",
    "                unique_templates_map[normalized_template_key] = {\n",
    "                    'EventId': f'HDFS_{event_id_hash}',\n",
    "                    'EventTemplate': normalized_template_key,\n",
    "                    'Description': 'Auto-generated template (Needs human review)',\n",
    "                    'SampleOriginalMessage': full_original_message,\n",
    "                    'SampleLevel': last_parsed_multi_line_header_info['level'],\n",
    "                    'SampleComponent': last_parsed_multi_line_header_info['component'],\n",
    "                }\n",
    "            final_parsed_entry = {\n",
    "                \"line_id_in_file_header\": last_parsed_multi_line_header_info['line_id_in_file_header'], \n",
    "                \"source_file\": last_parsed_multi_line_header_info['source_file'],\n",
    "                \"original_log_full\": full_original_message, \n",
    "                \"timestamp\": last_parsed_multi_line_header_info['timestamp'],\n",
    "                \"level\": last_parsed_multi_line_header_info['level'],\n",
    "                \"component\": last_parsed_multi_line_header_info['component'],\n",
    "                \"event_id\": unique_templates_map[normalized_template_key]['EventId'], \n",
    "                \"event_template\": normalized_template_key,\n",
    "                \"parameters\": \"\" \n",
    "            }\n",
    "            json_line = json.dumps(final_parsed_entry) + '\\n'\n",
    "            f_parsed_jsonl.write(json_line)\n",
    "            \n",
    "            offset_index_entry = {\n",
    "                'source_file': final_parsed_entry['source_file'],\n",
    "                'line_id_in_file_header': final_parsed_entry['line_id_in_file_header'],\n",
    "                'byte_offset': current_byte_offset # This is byte offset of the LAST entry\n",
    "            }\n",
    "            f_offset_index.write(json.dumps(offset_index_entry) + '\\n')\n",
    "            parsed_line_counter += 1\n",
    "\n",
    "    print(f\"\\nStreaming & parsing complete for {os.path.basename(log_file_to_stream)}.\")\n",
    "    print(f\"Total logical log entries parsed: {parsed_line_counter}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during streaming or parsing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aefd15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Saving Generated Templates to CSV ---\n",
      "Successfully saved 891 unique templates to ../data/templates/hdfs_v2_templates_stream_test.csv\n",
      "\n",
      "Example of generated templates (first 5):\n",
      "         EventId                                      EventTemplate  \\\n",
      "0  HDFS_B1BF6BD0                                       STARTUP_MSG:   \n",
      "1  HDFS_274E4A29  registered UNIX signal handlers for [<SIGNAL_L...   \n",
      "2  HDFS_A7699F29  loaded properties from hadoop-metrics<NUM>.pro...   \n",
      "3  HDFS_5EA7140F      Scheduled snapshot period at <NUM> second(s).   \n",
      "4  HDFS_EA28BB80                    DataNode metrics system started   \n",
      "\n",
      "                                    Description  \\\n",
      "0  Auto-generated template (Needs human review)   \n",
      "1  Auto-generated template (Needs human review)   \n",
      "2  Auto-generated template (Needs human review)   \n",
      "3  Auto-generated template (Needs human review)   \n",
      "4  Auto-generated template (Needs human review)   \n",
      "\n",
      "                               SampleOriginalMessage SampleLevel  \\\n",
      "0  2015-12-03 14:37:47,611 INFO org.apache.hadoop...        INFO   \n",
      "1  2015-12-03 14:37:47,618 INFO org.apache.hadoop...        INFO   \n",
      "2  2015-12-03 14:37:48,253 INFO org.apache.hadoop...        INFO   \n",
      "3  2015-12-03 14:37:48,315 INFO org.apache.hadoop...        INFO   \n",
      "4  2015-12-03 14:37:48,315 INFO org.apache.hadoop...        INFO   \n",
      "\n",
      "                                     SampleComponent  \n",
      "0    org.apache.hadoop.hdfs.server.datanode.DataNode  \n",
      "1    org.apache.hadoop.hdfs.server.datanode.DataNode  \n",
      "2      org.apache.hadoop.metrics2.impl.MetricsConfig  \n",
      "3  org.apache.hadoop.metrics2.impl.MetricsSystemImpl  \n",
      "4  org.apache.hadoop.metrics2.impl.MetricsSystemImpl  \n"
     ]
    }
   ],
   "source": [
    "print(\"--- Saving Generated Templates to CSV ---\")\n",
    "\n",
    "templates_data_list = []\n",
    "for template_key, template_info in unique_templates_map.items(): \n",
    "    templates_data_list.append(template_info)\n",
    "\n",
    "if templates_data_list:\n",
    "    templates_df = pd.DataFrame(templates_data_list)\n",
    "    templates_df = templates_df[['EventId', 'EventTemplate', 'Description', 'SampleOriginalMessage', 'SampleLevel', 'SampleComponent']]\n",
    "    templates_df.to_csv(OUTPUT_TEMPLATES_CSV, index=False)\n",
    "    print(f\"Successfully saved {len(templates_df)} unique templates to {OUTPUT_TEMPLATES_CSV}\")\n",
    "else:\n",
    "    print(\"No templates were generated. Check parsing process.\")\n",
    "\n",
    "print(f\"\\nExample of generated templates (first 5):\\n{templates_df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a72ed14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Sequence Retrieval with Live-Generated Index ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'offset_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Number of preceding logs to retrieve\u001b[39;00m\n\u001b[32m     14\u001b[39m NUM_LOGS_BEFORE = \u001b[32m5\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43moffset_map\u001b[49m: \u001b[38;5;66;03m# Ensure offset_map was populated\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to retrieve \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_LOGS_BEFORE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m preceding log entries for:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  File: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_problem_entry_metadata[\u001b[33m'\u001b[39m\u001b[33msource_file\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'offset_map' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Testing Sequence Retrieval with Live-Generated Index ---\")\n",
    "\n",
    "# --- IMPORTANT: MANUALLY PROVIDE A SAMPLE PROBLEMATIC ENTRY'S METADATA ---\n",
    "# You need to get this from the output of Cell 3's execution (console output).\n",
    "# Find an entry that says \"--- Analyzing Problem: ...\" and copy its\n",
    "# \"source_file\" and \"line_id_in_file_header\".\n",
    "# Adjust these values to one you see in your Cell 3 output.\n",
    "sample_problem_entry_metadata = {\n",
    "  \"source_file\": \"hadoop-hdfs-datanode-mesos-01.log\", # Example, please adjust for your data\n",
    "  \"line_id_in_file_header\": 80939 # Example, please adjust for your data\n",
    "} \n",
    "\n",
    "# Number of preceding logs to retrieve\n",
    "NUM_LOGS_BEFORE = 5 \n",
    "\n",
    "if offset_map: # Ensure offset_map was populated\n",
    "    print(f\"Attempting to retrieve {NUM_LOGS_BEFORE} preceding log entries for:\")\n",
    "    print(f\"  File: {sample_problem_entry_metadata['source_file']}\")\n",
    "    print(f\"  Line: {sample_problem_entry_metadata['line_id_in_file_header']}\")\n",
    "\n",
    "    log_sequence = get_contextual_log_sequence_from_disk(\n",
    "        all_parsed_jsonl_path=OUTPUT_PARSED_JSONL, # Path to the live-updated parsed JSONL\n",
    "        target_entry_metadata=sample_problem_entry_metadata, \n",
    "        num_lines_before=NUM_LOGS_BEFORE,\n",
    "        offset_map=offset_map # Use the in-memory offset_map populated in Cell 3\n",
    "    )\n",
    "\n",
    "    if log_sequence:\n",
    "        print(f\"\\n--- Retrieved Sequence ({len(log_sequence)} entries) ---\")\n",
    "        for entry in log_sequence:\n",
    "            print(f\"  [{entry.get('line_id_in_file_header', 'N/A')}] {entry.get('timestamp', 'N/A')} {entry.get('level', 'N/A')} {entry.get('component', 'N/A')}: {entry.get('original_log_full', '')[:100]}...\")\n",
    "        print(\"-----------------------------------\")\n",
    "    else:\n",
    "        print(\"No sequence retrieved. Check if target entry metadata is correct, or if it's too early in the log file.\")\n",
    "else:\n",
    "    print(\"Offset map not loaded/populated. Cannot test sequence retrieval.\")\n",
    "\n",
    "print(\"\\n--- Sequence Retrieval Test Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbb4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Building/Loading RAG Knowledge Base (with FAISS Persistence) ---\")\n",
    "\n",
    "# Check if FAISS index already exists on disk\n",
    "faiss_index_exists = os.path.exists(FAISS_INDEX_PATH) and os.listdir(FAISS_INDEX_PATH)\n",
    "\n",
    "if faiss_index_exists:\n",
    "    print(f\"Loading existing FAISS index from {FAISS_INDEX_PATH}...\")\n",
    "    try:\n",
    "        vectorstore = FAISS.load_local(FAISS_INDEX_PATH, embedding_model, allow_dangerous_deserialization=True) \n",
    "        print(\"FAISS index loaded successfully and contains data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index: {e}. Rebuilding from scratch.\")\n",
    "        faiss_index_exists = False \n",
    "\n",
    "if not faiss_index_exists: \n",
    "    print(\"No existing valid FAISS index found or loading failed. Building from scratch...\")\n",
    "    \n",
    "    # 1. Load Solution Documents\n",
    "    solution_doc_files_to_load = []\n",
    "    if not os.path.exists(SOLUTION_DOCS_DIR):\n",
    "        print(f\"ERROR: Solution documents directory not found at {SOLUTION_DOCS_DIR}. RAG will not have context.\")\n",
    "    else:\n",
    "        for root, _, files in os.walk(SOLUTION_DOCS_DIR):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file_path.endswith('.pdf'):\n",
    "                    loader = PyPDFLoader(file_path) \n",
    "                elif file_path.endswith(('.txt', '.md', '.html', '.docx', '.xlsx')):\n",
    "                    loader = UnstructuredFileLoader(file_path) \n",
    "                else:\n",
    "                    print(f\"Skipping unsupported file type: {file_path}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    docs = loader.load()\n",
    "                    solution_doc_files_to_load.extend(docs)\n",
    "                    print(f\"Loaded {len(docs)} pages/chunks from {os.path.basename(file_path)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Could not load {file_path}: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal raw documents loaded for RAG: {len(solution_doc_files_to_load)}\")\n",
    "    if not solution_doc_files_to_load:\n",
    "        print(\"WARNING: No solution documents were loaded. RAG will not have context.\")\n",
    "\n",
    "    # 2. Split Documents into Chunks\n",
    "    print(\"Splitting documents into smaller chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,     # Size of each chunk\n",
    "        chunk_overlap=200,   # Overlap between chunks to maintain context\n",
    "        length_function=len  # Use character length\n",
    "    )\n",
    "    rag_chunks = text_splitter.split_documents(solution_doc_files_to_load)\n",
    "    print(f\"Split into {len(rag_chunks)} chunks for RAG.\")\n",
    "\n",
    "    if not rag_chunks:\n",
    "        print(\"WARNING: No chunks created for RAG. Check document loading or text splitter settings.\")\n",
    "\n",
    "    # 3. Create Embeddings and Store in FAISS\n",
    "    print(f\"Creating embeddings and storing in FAISS (in-memory and saving to disk at {FAISS_INDEX_PATH})...\")\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=rag_chunks,\n",
    "        embedding=embedding_model\n",
    "    )\n",
    "    vectorstore.save_local(FAISS_INDEX_PATH) \n",
    "    print(\"Embeddings created and stored in FAISS, and saved to disk.\")\n",
    "\n",
    "# 4. Create Retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"RAG retriever initialized.\")\n",
    "\n",
    "print(\"\\n--- RAG Knowledge Base Setup Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Defining LLM Prompt Template ---\")\n",
    "\n",
    "# Define the Prompt Template for the LLM\n",
    "prompt_template = \"\"\"\n",
    "You are an expert HDFS (Hadoop Distributed File System) Site Reliability Engineer (SRE) and incident responder.\n",
    "Your task is to concisely analyze a given problematic HDFS log entry, understand the specific underlying problem, and generate a brief, actionable incident response plan.\n",
    "\n",
    "Here is the problematic HDFS log entry for analysis:\n",
    "{log_entry_full}\n",
    "\n",
    "Here is the sequence of preceding log events that led up to this problem:\n",
    "{sequence_of_events_json}\n",
    "\n",
    "Here is relevant troubleshooting and solution context from our knowledge base:\n",
    "{context}\n",
    "\n",
    "Based on the HDFS log entry, the sequence of events, and the provided context, please perform the following:\n",
    "\n",
    "1.  **Problem Summary:** Provide a concise, clear summary of what specifically went wrong, referencing insights from the sequence if possible.\n",
    "2.  **Severity Assessment:** Assign a severity level (Critical, High, Medium, Low) based on the log's impact.\n",
    "3.  **Root Cause Hypothesis:** Suggest the most probable root cause(s), specifically mentioning any causal links identified in the sequence.\n",
    "4.  **Affected Components:** List the HDFS components or services that are most likely affected.\n",
    "5.  **Actionable Response Plan (Role-Specific):**\n",
    "    * **DevOps/SRE Actions:** Detailed, step-by-step actions.\n",
    "    * **Developer Actions:** Specific areas to check in code, potential data issues, or configurations.\n",
    "    * **Security Actions (if applicable):** Steps to verify/address potential security implications.\n",
    "\n",
    "Format your entire response as a single JSON object with the following keys:\n",
    "\"summary\": \"...\",\n",
    "\"severity\": \"...\",\n",
    "\"root_cause_hypothesis\": \"...\",\n",
    "\"affected_components\": [\"...\", \"...\"],\n",
    "\"response_plan\": {{\n",
    "    \"devops_sre_actions\": [\"...\", \"...\"],\n",
    "    \"developer_actions\": [\"...\", \"...\"],\n",
    "    \"security_actions\": [\"...\", \"...\"]\n",
    "}}\n",
    "\n",
    "**CRITICAL INSTRUCTION:** Ensure your response is ONLY the complete and valid JSON object. Do NOT include any text before or after the JSON. Do NOT truncate the JSON object. It must be perfectly parsable JSON.\n",
    "\"\"\"\n",
    "\n",
    "# Create the PromptTemplate object\n",
    "PROMPT_TEMPLATE_LLM = PromptTemplate( # Renamed to avoid clash if 'PROMPT' is used by other LangChain components\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"log_entry_full\", \"sequence_of_events_json\", \"context\"]\n",
    ")\n",
    "\n",
    "print(\"LLM Prompt Template defined.\")\n",
    "\n",
    "print(\"\\n--- Running LLM Analysis & Solution Generation on Sample Problematic Entries ---\")\n",
    "\n",
    "# We will need to define a sample problematic entry here with source_file and line_id_in_file_header\n",
    "# as this notebook doesn't stream from the full 43GB file for all problematic entries.\n",
    "# This test is just to confirm the LLM/RAG/Sequence integration logic works.\n",
    "\n",
    "# IMPORTANT: You need to manually pick a sample entry from your 43GB JSONL file\n",
    "# (e.g., from an ERROR/WARN/FATAL entry) and copy its metadata here.\n",
    "# Example:\n",
    "sample_target_entry_metadata = {\n",
    "    \"source_file\": \"hadoop-hdfs-datanode-mesos-01.log\", \n",
    "    \"line_id_in_file_header\": 80939, # Example line ID from WARN: IOException in offerService\n",
    "    \"original_log_full\": \"2016-07-28 15:43:29,170 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService\\njava.io.EOFException: End of File Exception between local host is: \\\"mesos-master-1/10.10.34.11\\\"; destination host is: \\\"mesos-master-1\\\":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\\n...\"\n",
    "}\n",
    "\n",
    "if offset_map: # Ensure offset_map is loaded from Cell 2\n",
    "    print(f\"\\n--- Analyzing Sample Entry (ID: N/A, Level: {sample_target_entry_metadata.get('level', 'N/A')}) ---\")\n",
    "    print(f\"Original Full Log Snippet:\\n{sample_target_entry_metadata.get('original_log_full', '')[:500]}...\") \n",
    "\n",
    "    # 1. Retrieve Contextual Sequence (using the function from Cell 3)\n",
    "    log_sequence = get_contextual_log_sequence_from_disk(\n",
    "        all_parsed_jsonl_path=INPUT_PARSED_JSONL_FULL, \n",
    "        target_entry_metadata=sample_target_entry_metadata, \n",
    "        num_lines_before=NUM_PRECEDING_LOGS_FOR_SEQUENCE,\n",
    "        offset_map=offset_map \n",
    "    )\n",
    "    sequence_json_str = json.dumps(log_sequence, indent=2)\n",
    "    if not log_sequence:\n",
    "        sequence_json_str = \"No preceding log events retrieved or sequence unavailable.\"\n",
    "        print(\"Sequence analysis result: No preceding events retrieved.\")\n",
    "    else:\n",
    "        print(f\"Retrieved {len(log_sequence)} preceding log entries for sequence analysis.\")\n",
    "    \n",
    "    # 2. Prepare query for RAG retriever\n",
    "    retriever_query = f\"HDFS troubleshooting for {sample_target_entry_metadata.get('level', 'N/A')} log: {sample_target_entry_metadata.get('event_template', 'N/A')}. Original log snippet: {sample_target_entry_metadata.get('original_log_full', '')[:200]}\"\n",
    "    \n",
    "    # 3. Retrieve relevant context from knowledge base\n",
    "    print(\"Retrieving relevant context from knowledge base...\")\n",
    "    retrieved_docs = retriever.invoke(retriever_query)\n",
    "    \n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    if not context_text:\n",
    "        context_text = \"No specific context found in the knowledge base. The LLM will generate a general solution.\"\n",
    "        print(\"WARNING: No specific context retrieved from RAG. Solution may be general.\")\n",
    "    else:\n",
    "        print(f\"Retrieved {len(retrieved_docs)} document chunks.\")\n",
    "\n",
    "    # 4. Prepare the final prompt for the LLM\n",
    "    final_prompt_for_llm = PROMPT_TEMPLATE_LLM.format(\n",
    "        log_entry_full=sample_target_entry_metadata['original_log_full'], \n",
    "        sequence_of_events_json=sequence_json_str, \n",
    "        context=context_text\n",
    "    )\n",
    "\n",
    "    # 5. Invoke the LLM directly with the prepared prompt\n",
    "    print(\"Sending final prompt to Gemini LLM for solution generation...\")\n",
    "    try:\n",
    "        llm_response = llm.invoke(final_prompt_for_llm)\n",
    "        solution_json_str = llm_response.content.strip()\n",
    "\n",
    "        if solution_json_str.startswith(\"```json\"):\n",
    "            solution_json_str = solution_json_str.lstrip(\"```json\").rstrip(\"```\").strip()\n",
    "        \n",
    "        try:\n",
    "            generated_solution = json.loads(solution_json_str)\n",
    "            print(\"\\n--- GENERATED INCIDENT RESPONSE PLAN (JSON) ---\")\n",
    "            print(json.dumps(generated_solution, indent=2))\n",
    "            print(\"-----------------------------------------------\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"ERROR: LLM response was not valid JSON: {e}\")\n",
    "            print(f\"Raw LLM response content (first 500 chars):\\n {solution_json_str[:500]}\") \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to invoke LLM: {e}\")\n",
    "        print(\"Please check your API key, internet connection, and Gemini API quotas.\")\n",
    "\n",
    "else:\n",
    "    print(\"Offset map not loaded. Cannot proceed with LLM analysis.\")\n",
    "\n",
    "print(\"\\n--- LLM Analysis & Solution Generation Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
