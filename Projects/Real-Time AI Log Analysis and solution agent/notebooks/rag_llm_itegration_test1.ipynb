{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8de8cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini LLM initialized.\n",
      "Gemini Embedding model initialized.\n",
      "\n",
      "Full setup complete. Paths and models configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# LangChain components for RAG\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredFileLoader, UnstructuredMarkdownLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS # Using FAISS for persistent vector store\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv() # Load your API key from .env\n",
    "\n",
    "# Paths for data (relative to project root, as notebook is in 'notebooks/' folder)\n",
    "INPUT_PARSED_JSONL_FULL = \"../data/parsed_logs/hdfs_v2_parsed_hybrid_final.jsonl\" # The 43GB parsed log file\n",
    "INPUT_OFFSET_INDEX_CSV = \"../data/parsed_logs/hdfs_v2_offset_index.csv\" # The generated byte-offset index\n",
    "\n",
    "# Input for RAG knowledge base\n",
    "SOLUTION_DOCS_DIR = \"../data/solution_docs/\"\n",
    "\n",
    "# Path where FAISS index will persist its database (relative to project root)\n",
    "FAISS_INDEX_PATH = \"../faiss_index/\" \n",
    "\n",
    "# General parameters\n",
    "NUM_PRECEDING_LOGS_FOR_SEQUENCE = 20 # Number of lines before problematic one to retrieve for sequence\n",
    "MAX_PARSED_LOGS_TO_LOAD = 100000 # <--- Limit the number of parsed HDFS logs to load (from 43GB file)\n",
    "MAX_OFFSET_INDEX_LOAD = 100000 # <--- NEW: Limit the number of entries to load from byte-offset index CSV\n",
    "\n",
    "# --- LLM and Embedding Model Initialization ---\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\", \n",
    "    temperature=0.0, \n",
    "    max_output_tokens=4096, # Increased maximum output tokens\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "print(\"Gemini LLM initialized.\")\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\", \n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "print(\"Gemini Embedding model initialized.\")\n",
    "\n",
    "# Create necessary output directories (relative to project root)\n",
    "os.makedirs(os.path.dirname(INPUT_PARSED_JSONL_FULL), exist_ok=True) # Ensures data/parsed_logs\n",
    "os.makedirs(os.path.dirname(INPUT_OFFSET_INDEX_CSV), exist_ok=True) # Ensures data/parsed_logs\n",
    "os.makedirs(SOLUTION_DOCS_DIR, exist_ok=True) # Ensures data/solution_docs\n",
    "os.makedirs(FAISS_INDEX_PATH, exist_ok=True) # Ensures faiss_index/\n",
    "\n",
    "# Ensure input files for byte index and parsed logs exist\n",
    "if not os.path.exists(INPUT_PARSED_JSONL_FULL):\n",
    "    print(f\"ERROR: Full parsed JSONL file NOT FOUND at {INPUT_PARSED_JSONL_FULL}.\")\n",
    "    print(\"Please ensure you have run '01_hdfs_log_parsing_hybrid.ipynb' and copied the output.\")\n",
    "if not os.path.exists(INPUT_OFFSET_INDEX_CSV):\n",
    "    print(f\"ERROR: Byte-offset index CSV NOT FOUND at {INPUT_OFFSET_INDEX_CSV}.\")\n",
    "    print(f\"Please run '05_byte_offset_indexing.ipynb' first to create it.\")\n",
    "\n",
    "print(\"\\nFull setup complete. Paths and models configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04a4520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Byte-Offset Index ---\n",
      "Limiting byte-offset index loading to first 100000 entries for testing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6099f1277afc47e78c45ed6fd0fa4369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading offset map:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded byte-offset index with 100000 entries.\n",
      "\n",
      "--- Building RAG Knowledge Base ---\n",
      "Loading existing FAISS index from ../faiss_index/...\n",
      "FAISS index loaded successfully and contains data.\n",
      "RAG retriever initialized.\n",
      "\n",
      "--- RAG Knowledge Base Setup Complete ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Loading Byte-Offset Index ---\")\n",
    "offset_map = {} # Initialize empty map\n",
    "\n",
    "if os.path.exists(INPUT_OFFSET_INDEX_CSV):\n",
    "    try:\n",
    "        offset_df = pd.read_csv(INPUT_OFFSET_INDEX_CSV)\n",
    "        \n",
    "        # NEW: Limit the number of rows loaded from the byte-offset CSV\n",
    "        if len(offset_df) > MAX_OFFSET_INDEX_LOAD:\n",
    "            print(f\"Limiting byte-offset index loading to first {MAX_OFFSET_INDEX_LOAD} entries for testing.\")\n",
    "            offset_df = offset_df.head(MAX_OFFSET_INDEX_LOAD) # Use .head() to limit DataFrame rows\n",
    "\n",
    "        for index, row in tqdm(offset_df.iterrows(), total=len(offset_df), desc=\"Loading offset map\"):\n",
    "            offset_map[(str(row['source_file']), int(row['line_id_in_file_header']))] = int(row['byte_offset'])\n",
    "        print(f\"Loaded byte-offset index with {len(offset_map)} entries.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load offset index CSV: {e}\")\n",
    "        print(\"Please ensure '05_byte_offset_indexing.ipynb' ran successfully and generated a valid CSV.\")\n",
    "else:\n",
    "    print(\"WARNING: Byte-offset index CSV not found. Sequence retrieval will not work.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Building RAG Knowledge Base ---\")\n",
    "\n",
    "# Check if FAISS index already exists on disk\n",
    "faiss_index_exists = os.path.exists(FAISS_INDEX_PATH) and os.listdir(FAISS_INDEX_PATH)\n",
    "\n",
    "if faiss_index_exists:\n",
    "    print(f\"Loading existing FAISS index from {FAISS_INDEX_PATH}...\")\n",
    "    try:\n",
    "        vectorstore = FAISS.load_local(FAISS_INDEX_PATH, embedding_model, allow_dangerous_deserialization=True) \n",
    "        print(\"FAISS index loaded successfully and contains data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index: {e}. Rebuilding from scratch.\")\n",
    "        faiss_index_exists = False \n",
    "\n",
    "if not faiss_index_exists: \n",
    "    print(\"No existing valid FAISS index found or loading failed. Building from scratch...\")\n",
    "    \n",
    "    # 1. Load Solution Documents\n",
    "    solution_doc_files_to_load = []\n",
    "    if not os.path.exists(SOLUTION_DOCS_DIR):\n",
    "        print(f\"ERROR: Solution documents directory not found at {SOLUTION_DOCS_DIR}. RAG will not have context.\")\n",
    "    else:\n",
    "        for root, _, files in os.walk(SOLUTION_DOCS_DIR):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file_path.endswith('.pdf'):\n",
    "                    loader = PyPDFLoader(file_path) \n",
    "                elif file_path.endswith(('.txt', '.md', '.html', '.docx', '.xlsx')):\n",
    "                    loader = UnstructuredFileLoader(file_path) \n",
    "                else:\n",
    "                    print(f\"Skipping unsupported file type: {file_path}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    docs = loader.load()\n",
    "                    solution_doc_files_to_load.extend(docs)\n",
    "                    print(f\"Loaded {len(docs)} pages/chunks from {os.path.basename(file_path)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Could not load {file_path}: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal raw documents loaded for RAG: {len(solution_doc_files_to_load)}\")\n",
    "    if not solution_doc_files_to_load:\n",
    "        print(\"WARNING: No solution documents were loaded. RAG will not have context.\")\n",
    "\n",
    "    # 2. Split Documents into Chunks\n",
    "    print(\"Splitting documents into smaller chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,     # Size of each chunk\n",
    "        chunk_overlap=200,   # Overlap between chunks to maintain context\n",
    "        length_function=len  # Use character length\n",
    "    )\n",
    "    rag_chunks = text_splitter.split_documents(solution_doc_files_to_load)\n",
    "    \n",
    "    # OLD LIMIT - REMOVED: if len(rag_chunks) > MAX_RAG_CHUNKS_TO_LOAD:\n",
    "    #     print(f\"Limiting RAG chunks to first {MAX_RAG_CHUNKS_TO_LOAD} for testing.\")\n",
    "    #     rag_chunks = rag_chunks[:MAX_RAG_CHUNKS_TO_LOAD]\n",
    "\n",
    "    print(f\"Split into {len(rag_chunks)} chunks for RAG.\")\n",
    "\n",
    "    if not rag_chunks:\n",
    "        print(\"WARNING: No chunks created for RAG. Check document loading or text splitter settings.\")\n",
    "\n",
    "    # 3. Create Embeddings and Store in FAISS\n",
    "    print(f\"Creating embeddings and storing in FAISS (in-memory and saving to disk at {FAISS_INDEX_PATH})...\")\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=rag_chunks,\n",
    "        embedding=embedding_model\n",
    "    )\n",
    "    vectorstore.save_local(FAISS_INDEX_PATH) # Save FAISS index to disk\n",
    "    print(\"Embeddings created and stored in FAISS, and saved to disk.\")\n",
    "\n",
    "# 4. Create Retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"RAG retriever initialized.\")\n",
    "\n",
    "print(\"\\n--- RAG Knowledge Base Setup Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72e5377f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence retrieval function defined.\n",
      "\n",
      "--- Loading & Filtering Parsed HDFS Logs for LLM Analysis ---\n",
      "Loading first 100000 lines from ../data/parsed_logs/hdfs_v2_parsed_hybrid_final.jsonl for testing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604fcb4564904697877c9d62962a9966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading up to 100000 parsed logs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 100000 total parsed HDFS log entries into memory (for sequence lookup).\n",
      "Found 11 problematic log entries for LLM analysis.\n",
      "\n",
      "--- Parsed HDFS Logs Loaded & Problematic Entries Filtered (Sample) ---\n"
     ]
    }
   ],
   "source": [
    "# --- Define Sequence Retrieval Function ---\n",
    "def get_contextual_log_sequence_from_disk(\n",
    "    all_parsed_jsonl_path: str, # Path to the 43GB JSONL file\n",
    "    target_entry_metadata: dict, # Problematic entry's metadata: {'source_file': ..., 'line_id_in_file_header': ...}\n",
    "    num_lines_before: int, \n",
    "    offset_map: dict # The loaded (source_file, line_id) -> byte_offset map\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Retrieves a sequence of log entries preceding a target problematic entry from the JSONL file on disk.\n",
    "    Uses the byte-offset map for efficient seeking.\n",
    "    \"\"\"\n",
    "    target_source_file = target_entry_metadata.get('source_file')\n",
    "    target_line_id = target_entry_metadata.get('line_id_in_file_header')\n",
    "\n",
    "    if not target_source_file or not isinstance(target_line_id, int):\n",
    "        print(f\"Error in get_contextual_log_sequence_from_disk: Missing 'source_file' or 'line_id_in_file_header' in target_entry_metadata.\")\n",
    "        return []\n",
    "\n",
    "    sequence = []\n",
    "    \n",
    "    # Calculate the start line ID for the sequence\n",
    "    start_line_id_in_file = max(1, target_line_id - num_lines_before)\n",
    "    \n",
    "    # Attempt to find the byte offset for the start of the sequence\n",
    "    # Offset map key is (source_file, line_id_in_file_header)\n",
    "    start_offset_key = (target_source_file, start_line_id_in_file)\n",
    "    start_byte_offset = offset_map.get(start_offset_key)\n",
    "\n",
    "    if start_byte_offset is None:\n",
    "        print(f\"Warning: Exact start offset for line {start_line_id_in_file} in {target_source_file} not found in offset map. Returning empty sequence.\")\n",
    "        return [] \n",
    "\n",
    "    # Read lines from the parsed JSONL file on disk\n",
    "    try:\n",
    "        with open(all_parsed_jsonl_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            f.seek(start_byte_offset) # Jump directly to the approximate start of the sequence\n",
    "\n",
    "            lines_read_from_seek = 0\n",
    "            # Read enough lines to cover the sequence, plus some buffer, until target line_id\n",
    "            for line in f:\n",
    "                if lines_read_from_seek >= num_lines_before * 2 + 5: # Read up to 2*N lines plus buffer to be safe\n",
    "                    break \n",
    "\n",
    "                try:\n",
    "                    parsed_seq_entry = json.loads(line)\n",
    "                    # Add only logs from the same source file and BEFORE the target line_id\n",
    "                    if (parsed_seq_entry.get('source_file') == target_source_file and \n",
    "                        parsed_seq_entry.get('line_id_in_file_header', 0) < target_line_id):\n",
    "                        sequence.append(parsed_seq_entry)\n",
    "                    lines_read_from_seek += 1\n",
    "                except json.JSONDecodeError:\n",
    "                    pass \n",
    "\n",
    "        # Ensure the sequence is sorted by line_id (chronological) and limited to num_lines_before\n",
    "        sequence.sort(key=lambda x: x.get('line_id_in_file_header', 0))\n",
    "        sequence = sequence[-num_lines_before:] # Take the last N lines, which are the preceding ones\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving sequence from disk for {target_source_file} line {target_line_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "    return sequence\n",
    "\n",
    "print(\"Sequence retrieval function defined.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Loading & Filtering Parsed HDFS Logs for LLM Analysis ---\")\n",
    "\n",
    "# Load only the first MAX_PARSED_LOGS_TO_LOAD entries for testing\n",
    "problematic_log_entries_for_llm = [] \n",
    "all_parsed_logs_for_sequence_lookup = [] # This list will hold all parsed entries (up to MAX_PARSED_LOGS_TO_LOAD)\n",
    "PROBLEMATIC_LEVELS_TO_ANALYZE = ['ERROR', 'WARN', 'FATAL']\n",
    "\n",
    "if os.path.exists(INPUT_PARSED_JSONL_FULL):\n",
    "    print(f\"Loading first {MAX_PARSED_LOGS_TO_LOAD} lines from {INPUT_PARSED_JSONL_FULL} for testing...\")\n",
    "    with open(INPUT_PARSED_JSONL_FULL, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        # Use tqdm to show progress for this loading loop\n",
    "        for i, line in enumerate(tqdm(f, desc=f\"Loading up to {MAX_PARSED_LOGS_TO_LOAD} parsed logs\")):\n",
    "            if i >= MAX_PARSED_LOGS_TO_LOAD: # Stop after MAX_PARSED_LOGS_TO_LOAD lines\n",
    "                break\n",
    "            try:\n",
    "                parsed_entry = json.loads(line)\n",
    "                all_parsed_logs_for_sequence_lookup.append(parsed_entry) # Store all loaded for sequence lookup\n",
    "                \n",
    "                if 'level' in parsed_entry and parsed_entry['level'].strip().upper() in PROBLEMATIC_LEVELS_TO_ANALYZE:\n",
    "                    # Store its original index in this loaded list for sequence retrieval\n",
    "                    parsed_entry['original_index_in_loaded_list'] = i \n",
    "                    problematic_log_entries_for_llm.append(parsed_entry)\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Could not decode JSON on line {i+1} of parsed logs: {e} - Line: {line.strip()[:100]}...\")\n",
    "                \n",
    "    print(f\"Successfully loaded {len(all_parsed_logs_for_sequence_lookup)} total parsed HDFS log entries into memory (for sequence lookup).\")\n",
    "    print(f\"Found {len(problematic_log_entries_for_llm)} problematic log entries for LLM analysis.\")\n",
    "else:\n",
    "    print(f\"ERROR: Full parsed JSONL file NOT FOUND at {INPUT_PARSED_JSONL_FULL}.\")\n",
    "\n",
    "if not problematic_log_entries_for_llm:\n",
    "    print(\"WARNING: No problematic log entries loaded in the sample. LLM analysis will be skipped.\")\n",
    "\n",
    "print(\"\\n--- Parsed HDFS Logs Loaded & Problematic Entries Filtered (Sample) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e672c163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining LLM Prompt Template ---\n",
      "LLM Prompt Template defined.\n",
      "\n",
      "--- LLM Analysis & Solution Generation ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb3b5887cb7420893fc3f65c9b0f429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing problematic entries:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Entry 1 (ID: HDFS_6C708D31, Level: WARN) ---\n",
      "Event Template: Slow BlockReceiver write packet to mirror took <NUM>ms (threshold=<NUM>ms)\n",
      "Original Full Log Snippet:\n",
      "2016-04-13 21:56:12,682 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 340ms (threshold=300ms)...\n",
      "Retrieved 20 preceding log entries for sequence analysis.\n",
      "Retrieving relevant context from knowledge base...\n",
      "Retrieved 4 document chunks.\n",
      "Sending final prompt to Gemini LLM for solution generation...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 125\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSending final prompt to Gemini LLM for solution generation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     llm_response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_prompt_for_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     solution_json_str = llm_response.content.strip()\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m solution_json_str.startswith(\u001b[33m\"\u001b[39m\u001b[33m```json\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:158\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    149\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     **kwargs: Any,\n\u001b[32m    154\u001b[39m ) -> BaseMessage:\n\u001b[32m    155\u001b[39m     config = ensure_config(config)\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    157\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    168\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:560\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    553\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    554\u001b[39m     prompts: List[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    557\u001b[39m     **kwargs: Any,\n\u001b[32m    558\u001b[39m ) -> LLMResult:\n\u001b[32m    559\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:421\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    420\u001b[39m             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    422\u001b[39m flattened_outputs = [\n\u001b[32m    423\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    425\u001b[39m ]\n\u001b[32m    426\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:411\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    410\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m         )\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:632\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    636\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:555\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    544\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    545\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    548\u001b[39m     **kwargs: Any,\n\u001b[32m    549\u001b[39m ) -> ChatResult:\n\u001b[32m    550\u001b[39m     params, chat, message = \u001b[38;5;28mself\u001b[39m._prepare_chat(\n\u001b[32m    551\u001b[39m         messages,\n\u001b[32m    552\u001b[39m         stop=stop,\n\u001b[32m    553\u001b[39m         **kwargs,\n\u001b[32m    554\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m     response: genai.types.GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:152\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:336\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    334\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    335\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:398\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:134\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\generativeai\\generative_models.py:426\u001b[39m, in \u001b[36mChatSession.send_message\u001b[39m\u001b[34m(self, content, generation_config, safety_settings, stream, tools)\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config.get(\u001b[33m\"\u001b[39m\u001b[33mcandidate_count\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt chat with `candidate_count > 1`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28mself\u001b[39m._check_response(response=response, stream=stream)\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enable_automatic_function_calling \u001b[38;5;129;01mand\u001b[39;00m tools_lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\generativeai\\generative_models.py:232\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, request_options)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:566\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    561\u001b[39m metadata = \u001b[38;5;28mtuple\u001b[39m(metadata) + (\n\u001b[32m    562\u001b[39m     gapic_v1.routing_header.to_grpc_metadata(((\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, request.model),)),\n\u001b[32m    563\u001b[39m )\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\grpc\\_channel.py:1178\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1166\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1168\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1173\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1174\u001b[39m ) -> Any:\n\u001b[32m   1175\u001b[39m     (\n\u001b[32m   1176\u001b[39m         state,\n\u001b[32m   1177\u001b[39m         call,\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\grpc\\_channel.py:1162\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1145\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1146\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1147\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1160\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1161\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1163\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"--- Defining LLM Prompt Template ---\")\n",
    "\n",
    "# Define the Prompt Template for the LLM\n",
    "# FINALIZED: Adds meta-reasoning, causal flow in root cause, and impact of solution steps.\n",
    "prompt_template = \"\"\"\n",
    "You are an expert HDFS (Hadoop Distributed File System) Site Reliability Engineer (SRE) and incident responder.\n",
    "Your task is to concisely analyze a given problematic HDFS log entry, understand the specific underlying problem, and generate a brief, actionable incident response plan.\n",
    "\n",
    "Here is the problematic HDFS log entry for analysis:\n",
    "{log_entry_full}\n",
    "\n",
    "Here is the sequence of preceding log events that led up to this problem:\n",
    "{sequence_of_events_json}\n",
    "\n",
    "Here is relevant troubleshooting and solution context from our knowledge base:\n",
    "{context}\n",
    "\n",
    "Based on the HDFS log entry, the sequence of events, and the provided context, please perform the following:\n",
    "\n",
    "**LLM Analysis Feedback (Self-Evaluation):**\n",
    "* **Confidence Level (High/Medium/Low):** How confident are you in this analysis based on the provided information?\n",
    "* **Context Sufficiency (Sufficient/Partially Sufficient/Insufficient):** Is the provided `log_entry_full`, `sequence_of_events_json`, and `context` enough for a definitive root cause and solution?\n",
    "* **Needed Additional Info (If Insufficient):** Explain what specific additional log entries (e.g., timestamps, components, error messages), metrics, or external context would improve accuracy.\n",
    "\n",
    "1.  **Problem Summary (Concise):** Provide a brief, clear summary of what specifically went wrong, referencing insights from the sequence if possible.\n",
    "2.  **Severity Assessment:** Assign a severity level (Critical, High, Medium, Low) based on the log's impact.\n",
    "3.  **Root Cause Hypothesis:** Suggest the most probable root cause(s). **Explain the flow or chain of events that likely led to this problem, explicitly referencing specific events or patterns from the provided sequence.** Be specific about the modules or components involved in the causal chain.\n",
    "4.  **Affected Components:** List the HDFS components or services that are most likely affected.\n",
    "5.  **Actionable Response Plan (Brief & Role-Specific):**\n",
    "    * For each action, provide:\n",
    "        * `step_description`: A concise description of the action.\n",
    "        * `responsible_team`: Specify WHICH TEAM (DevOps/SRE, Developer, Security).\n",
    "        * `responsible_module_or_component`: Specify WHICH MODULE/COMPONENT should perform the action (e.g., DataNode Storage, NameNode IPC Layer, HDFS Client Library).\n",
    "        * `specific_effect_on_problem`: What this action will specifically do and how it directly addresses the error or root cause.\n",
    "        * `expected_outcome_or_status`: What specific result to look for if the action is successful (e.g., \"logs show 'service started'\", \"metric returns to baseline\").\n",
    "    * **Limit the number of actions to the top 3-5 most impactful per role.**\n",
    "    * **Do NOT include direct commands in the `step_description`.** Focus on the action and its specific impact.\n",
    "\n",
    "Format your entire response as a single JSON object with the following keys:\n",
    "\"llm_analysis_feedback\": {{\n",
    "    \"confidence_level\": \"...\",\n",
    "    \"context_sufficiency\": \"...\",\n",
    "    \"needed_additional_info\": \"...\"\n",
    "}},\n",
    "\"summary\": \"...\",\n",
    "\"severity\": \"...\",\n",
    "\"root_cause_hypothesis\": \"...\",\n",
    "\"affected_components\": [\"...\", \"...\"],\n",
    "\"response_plan\": {{\n",
    "    \"devops_sre_actions\": [\n",
    "        {{\"step_description\": \"...\", \"responsible_team\": \"...\", \"responsible_module_or_component\": \"...\", \"specific_effect_on_problem\": \"...\", \"expected_outcome_or_status\": \"...\"}},\n",
    "        {{\"step_description\": \"...\", \"responsible_team\": \"...\", \"responsible_module_or_component\": \"...\", \"specific_effect_on_problem\": \"...\", \"expected_outcome_or_status\": \"...\"}}\n",
    "    ],\n",
    "    \"developer_actions\": [\n",
    "        {{\"step_description\": \"...\", \"responsible_team\": \"...\", \"responsible_module_or_component\": \"...\", \"specific_effect_on_problem\": \"...\", \"expected_outcome_or_status\": \"...\"}},\n",
    "        {{\"step_description\": \"...\", \"responsible_team\": \"...\", \"responsible_module_or_component\": \"...\", \"specific_effect_on_problem\": \"...\", \"expected_outcome_or_status\": \"...\"}}\n",
    "    ],\n",
    "    \"security_actions\": [\n",
    "        {{\"step_description\": \"...\", \"responsible_team\": \"...\", \"responsible_module_or_component\": \"...\", \"specific_effect_on_problem\": \"...\", \"expected_outcome_or_status\": \"...\"}},\n",
    "        {{\"step_description\": \"...\", \"responsible_team\": \"...\", \"responsible_module_or_component\": \"...\", \"specific_effect_on_problem\": \"...\", \"expected_outcome_or_status\": \"...\"}}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "**CRITICAL INSTRUCTION:** Ensure your response is ONLY the complete and valid JSON object. Do NOT include any text before or after the JSON. Do NOT truncate the JSON object. It must be perfectly parsable JSON.\n",
    "\"\"\"\n",
    "\n",
    "# Create the PromptTemplate object\n",
    "PROMPT_TEMPLATE_LLM = PromptTemplate( # Renamed to avoid clash if 'PROMPT' is used by other LangChain components\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"log_entry_full\", \"sequence_of_events_json\", \"context\"]\n",
    ")\n",
    "\n",
    "print(\"LLM Prompt Template defined.\")\n",
    "print(\"\\n--- LLM Analysis & Solution Generation ---\")\n",
    "\n",
    "if problematic_log_entries_for_llm: # Iterate through samples loaded in Cell 3\n",
    "    # IMPORTANT: For quick testing, you might want to slice this list (e.g., problematic_log_entries_for_llm[:5])\n",
    "    for i, entry_to_analyze in enumerate(tqdm(problematic_log_entries_for_llm, desc=\"Analyzing problematic entries\")):\n",
    "        print(f\"\\n--- Analyzing Entry {i+1} (ID: {entry_to_analyze.get('event_id', 'N/A')}, Level: {entry_to_analyze.get('level', 'N/A')}) ---\")\n",
    "        print(f\"Event Template: {entry_to_analyze.get('event_template', 'N/A')}\")\n",
    "        print(f\"Original Full Log Snippet:\\n{entry_to_analyze.get('original_log_full', '')[:500]}...\") \n",
    "\n",
    "        # 1. Retrieve Contextual Sequence (using the function from Cell 3)\n",
    "        # This function is defined in Cell 3, which is now executing before this cell.\n",
    "        # It relies on 'all_parsed_logs_for_sequence_lookup' (from Cell 3) and 'offset_map' (from Cell 2).\n",
    "        # Both of these are loaded at the beginning of this notebook.\n",
    "        \n",
    "        log_sequence = get_contextual_log_sequence_from_disk(\n",
    "            all_parsed_jsonl_path=INPUT_PARSED_JSONL_FULL, # This path is for the f.seek()\n",
    "            target_entry_metadata=entry_to_analyze, \n",
    "            num_lines_before=NUM_PRECEDING_LOGS_FOR_SEQUENCE,\n",
    "            offset_map=offset_map # From Cell 2\n",
    "        )\n",
    "        sequence_json_str = json.dumps(log_sequence, indent=2)\n",
    "        if not log_sequence:\n",
    "            sequence_json_str = \"No preceding log events retrieved or sequence unavailable.\"\n",
    "            print(\"Sequence analysis result: No preceding events retrieved.\")\n",
    "        else:\n",
    "            print(f\"Retrieved {len(log_sequence)} preceding log entries for sequence analysis.\")\n",
    "        \n",
    "        # 2. Prepare query for RAG retriever\n",
    "        retriever_query = f\"HDFS troubleshooting for {entry_to_analyze.get('level', 'N/A')} log: {entry_to_analyze.get('event_template', '')}. Original log snippet: {entry_to_analyze.get('original_log_full', '')[:200]}\"\n",
    "        \n",
    "        # 3. Retrieve relevant context from knowledge base\n",
    "        print(\"Retrieving relevant context from knowledge base...\")\n",
    "        retrieved_docs = retriever.invoke(retriever_query)\n",
    "        \n",
    "        context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        if not context_text:\n",
    "            context_text = \"No specific context found in the knowledge base. The LLM will generate a general solution.\"\n",
    "            print(\"WARNING: No specific context retrieved from RAG. Solution may be general.\")\n",
    "        else:\n",
    "            print(f\"Retrieved {len(retrieved_docs)} document chunks.\")\n",
    "\n",
    "        # 4. Prepare the final prompt for the LLM\n",
    "        final_prompt_for_llm = PROMPT_TEMPLATE_LLM.format(\n",
    "            log_entry_full=entry_to_analyze['original_log_full'], \n",
    "            sequence_of_events_json=sequence_json_str, \n",
    "            context=context_text\n",
    "        )\n",
    "\n",
    "        # 5. Invoke the LLM directly with the prepared prompt\n",
    "        print(\"Sending final prompt to Gemini LLM for solution generation...\")\n",
    "        try:\n",
    "            llm_response = llm.invoke(final_prompt_for_llm)\n",
    "            solution_json_str = llm_response.content.strip()\n",
    "\n",
    "            if solution_json_str.startswith(\"```json\"):\n",
    "                solution_json_str = solution_json_str.lstrip(\"```json\").rstrip(\"```\").strip()\n",
    "            \n",
    "            try:\n",
    "                generated_solution = json.loads(solution_json_str)\n",
    "                print(\"\\n--- GENERATED INCIDENT RESPONSE PLAN (JSON) ---\")\n",
    "                print(json.dumps(generated_solution, indent=2))\n",
    "                print(\"-----------------------------------------------\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"ERROR: LLM response was not valid JSON for entry {entry_to_analyze.get('event_id', 'N/A')}: {e}\")\n",
    "                print(f\"Raw LLM response content (first 500 chars):\\n {solution_json_str[:500]}\") \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to invoke LLM for entry {entry_to_analyze.get('event_id', 'N/A')}: {e}\")\n",
    "            print(\"Please check your API key, internet connection, and Gemini API quotas.\")\n",
    "\n",
    "else:\n",
    "    print(\"No problematic templates found to analyze. Please ensure Cell 3 loaded some.\")\n",
    "\n",
    "print(\"\\n--- Full LLM Analysis & Solution Generation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d544e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining LLM Prompt Template ---\n",
      "LLM Prompt Template defined.\n",
      "\n",
      "--- LLM Analysis & Solution Generation ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'problematic_log_entries_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLLM Prompt Template defined.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- LLM Analysis & Solution Generation ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mproblematic_log_entries_full\u001b[49m: \u001b[38;5;66;03m# Iterate through samples loaded in Cell 3\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# IMPORTANT: For quick testing, you might want to slice this list (e.g., problematic_log_entries_full[:5])\u001b[39;00m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, entry_to_analyze \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(problematic_log_entries_full, desc=\u001b[33m\"\u001b[39m\u001b[33mAnalyzing problematic entries\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m     87\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Analyzing Entry \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry_to_analyze.get(\u001b[33m'\u001b[39m\u001b[33mevent_id\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Level: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry_to_analyze.get(\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'problematic_log_entries_full' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"--- Defining LLM Prompt Template ---\")\n",
    "\n",
    "# Define the Prompt Template for the LLM\n",
    "# FINALIZED: Adds meta-reasoning, causal flow in root cause, and impact of solution steps.\n",
    "prompt_template = \"\"\"\n",
    "You are an expert HDFS (Hadoop Distributed File System) Site Reliability Engineer (SRE) and incident responder.\n",
    "Your task is to concisely analyze a given problematic HDFS log entry, understand the specific underlying problem, and generate a brief, actionable incident response plan.\n",
    "\n",
    "Here is the problematic HDFS log entry for analysis:\n",
    "{log_entry_full}\n",
    "\n",
    "Here is the sequence of preceding log events that led up to this problem:\n",
    "{sequence_of_events_json}\n",
    "\n",
    "Here is relevant troubleshooting and solution context from our knowledge base:\n",
    "{context}\n",
    "\n",
    "Based on the HDFS log entry, the sequence of events, and the provided context, please perform the following:\n",
    "\n",
    "**LLM Analysis Feedback (Self-Evaluation):**\n",
    "* **Confidence Level (High/Medium/Low):** How confident are you in this analysis based on the provided information?\n",
    "* **Context Sufficiency (Sufficient/Partially Sufficient/Insufficient):** Is the provided `log_entry_full`, `sequence_of_events_json`, and `context` enough for a definitive root cause and solution?\n",
    "* **Needed Additional Info (If Insufficient):** Explain what specific additional log entries (e.g., timestamps, components, error messages), metrics, or external context would improve accuracy.\n",
    "\n",
    "1.  **Problem Summary (Concise):** Provide a brief, clear summary of what specifically went wrong, referencing insights from the sequence if possible.\n",
    "2.  **Severity Assessment:** Assign a severity level (Critical, High, Medium, Low) based on the log's impact.\n",
    "3.  **Impact Assessment (Brief):** Briefly describe the potential impact on HDFS services or users (e.g., data durability, performance, availability).\n",
    "4.  **Root Cause Hypothesis:** Suggest the most probable root cause(s). **Explain the flow or chain of events that likely led to this problem, explicitly referencing specific events or patterns from the provided sequence.** Be specific about the modules or components involved in the causal chain.\n",
    "5.  **Affected Components:** List the HDFS components or services that are most likely affected.\n",
    "6.  **Actionable Response Plan (Brief & Role-Specific):**\n",
    "    * For each action, provide:\n",
    "        * `step_description`: A concise description of the action.\n",
    "        * `responsible_team`: Specify WHICH TEAM (DevOps/SRE, Developer, Security).\n",
    "        * `responsible_module_or_component`: Specify WHICH MODULE/COMPONENT should perform the action (e.g., DataNode Storage, NameNode IPC Layer, HDFS Client Library).\n",
    "        * `specific_effect_on_problem`: What this action will specifically do and how it directly addresses the error or root cause.\n",
    "        * `expected_outcome_or_status`: What specific result to look for if the action is successful (e.g., \"logs show 'service started'\", \"metric returns to baseline\").\n",
    "        * `type`: Classify as 'DIAGNOSTIC_ONLY' or 'POTENTIALLY_MODIFIES_STATE'.\n",
    "    * **Prioritize the top 3-5 most impactful actions per role.**\n",
    "    * **Do NOT include direct commands as the 'command' field is not part of this structure.** Focus on the action and its specific impact.\n",
    "\n",
    "7.  **Known Workarounds/Temporary Mitigations (If Applicable):**\n",
    "    * Provide a brief list of any quick, temporary fixes that could alleviate the immediate problem while a permanent solution is being investigated. If none, state \"None identified\".\n",
    "\n",
    "Format your entire response as a single JSON object with the following keys:\n",
    "\"llm_analysis_feedback\": {{\n",
    "    \"confidence_level\": \"...\",\n",
    "    \"context_sufficiency\": \"...\",\n",
    "    \"needed_additional_info\": \"...\"\n",
    "}},\n",
    "\"summary\": \"...\",\n",
    "\"severity\": \"...\",\n",
    "\"impact_assessment\": \"...\", # <--- NEW FIELD\n",
    "\"root_cause_hypothesis\": \"...\",\n",
    "\"affected_components\": [\"...\", \"...\"],\n",
    "\"response_plan\": {{\n",
    "    \"devops_sre_actions\": [\n",
    "        {{\"step_description\": \"...\", \"responsible_team\": \"...\", \"responsible_module_or_component\": \"...\", \"specific_effect_on_problem\": \"...\", \"expected_outcome_or_status\": \"...\", \"type\": \"...\"}},\n",
    "        {{\"step_description\": \"...\", \"responsible_team\": \"...\", \"responsible_module_or_component\": \"...\", \"specific_effect_on_problem\": \"...\", \"expected_outcome_or_status\": \"...\", \"type\": \"...\"}}\n",
    "    ],\n",
    "    \"developer_actions\": [\n",
    "        {{\"step_description\": \"...\", \"responsible_team\": \"...\", \"responsible_module_or_component\": \"...\", \"specific_effect_on_problem\": \"...\", \"expected_outcome_or_status\": \"...\", \"type\": \"...\"}},\n",
    "        {{\"step_description\": \"...\", \"responsible_team\": \"...\", \"responsible_module_or_component\": \"...\", \"specific_effect_on_problem\": \"...\", \"expected_outcome_or_status\": \"...\", \"type\": \"...\"}}\n",
    "    ],\n",
    "    \"security_actions\": [\n",
    "        {{\"step_description\": \"...\", \"responsible_team\": \"...\", \"responsible_module_or_component\": \"...\", \"specific_effect_on_problem\": \"...\", \"expected_outcome_or_status\": \"...\", \"type\": \"...\"}},\n",
    "        {{\"step_description\": \"...\", \"responsible_team\": \"...\", \"responsible_module_or_component\": \"...\", \"specific_effect_on_problem\": \"...\", \"expected_outcome_or_status\": \"...\", \"type\": \"...\"}}\n",
    "    ]\n",
    "}},\n",
    "\"temporary_mitigations\": [\"...\", \"...\"] # <--- NEW FIELD\n",
    "}}\n",
    "\n",
    "**CRITICAL INSTRUCTION:** Ensure your response is ONLY the complete and valid JSON object. Do NOT include any text before or after the JSON. Do NOT truncate the JSON object. It must be perfectly parsable JSON.\n",
    "\"\"\"\n",
    "\n",
    "# Create the PromptTemplate object\n",
    "PROMPT_TEMPLATE_LLM = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"log_entry_full\", \"sequence_of_events_json\", \"context\"]\n",
    ")\n",
    "\n",
    "print(\"LLM Prompt Template defined.\")\n",
    "print(\"\\n--- LLM Analysis & Solution Generation ---\")\n",
    "\n",
    "if problematic_log_entries_for_llm: # Iterate through samples loaded in Cell 3\n",
    "    # IMPORTANT: For quick testing, you might want to slice this list (e.g., problematic_log_entries_full[:5])\n",
    "    for i, entry_to_analyze in enumerate(tqdm(problematic_log_entries_for_llm, desc=\"Analyzing problematic entries\")):\n",
    "        print(f\"\\n--- Analyzing Entry {i+1} (ID: {entry_to_analyze.get('event_id', 'N/A')}, Level: {entry_to_analyze.get('level', 'N/A')}) ---\")\n",
    "        print(f\"Event Template: {entry_to_analyze.get('event_template', 'N/A')}\")\n",
    "        print(f\"Original Full Log Snippet:\\n{entry_to_analyze.get('original_log_full', '')[:500]}...\") \n",
    "\n",
    "        # 1. Retrieve Contextual Sequence\n",
    "        log_sequence = get_contextual_log_sequence_from_disk(\n",
    "            all_parsed_jsonl_path=INPUT_PARSED_JSONL_FULL, \n",
    "            target_entry_metadata=entry_to_analyze, \n",
    "            num_lines_before=NUM_PRECEDING_LOGS_FOR_SEQUENCE,\n",
    "            offset_map=offset_map \n",
    "        )\n",
    "        sequence_json_str = json.dumps(log_sequence, indent=2)\n",
    "        if not log_sequence:\n",
    "            sequence_json_str = \"No preceding log events retrieved or sequence unavailable.\"\n",
    "            print(\"Sequence analysis result: No preceding events retrieved.\")\n",
    "        else:\n",
    "            print(f\"Retrieved {len(log_sequence)} preceding log entries for sequence analysis.\")\n",
    "        \n",
    "        # 2. Prepare query for RAG retriever\n",
    "        retriever_query = f\"HDFS troubleshooting for {entry_to_analyze.get('level', 'N/A')} log: {entry_to_analyze.get('event_template', '')}. Provide solution for original message: {entry_to_analyze.get('original_log_full', '')[:200]}\"\n",
    "        \n",
    "        # 3. Retrieve relevant context from knowledge base\n",
    "        print(\"Retrieving relevant context from knowledge base...\")\n",
    "        retrieved_docs = retriever.invoke(retriever_query)\n",
    "        \n",
    "        context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        if not context_text:\n",
    "            context_text = \"No specific context found in the knowledge base. The LLM will generate a general solution.\"\n",
    "            print(\"WARNING: No specific context retrieved from RAG. Solution may be general.\")\n",
    "        else:\n",
    "            print(f\"Retrieved {len(retrieved_docs)} document chunks.\")\n",
    "\n",
    "        # 4. Prepare the final prompt for the LLM\n",
    "        final_prompt_for_llm = PROMPT_TEMPLATE_LLM.format(\n",
    "            log_entry_full=entry_to_analyze['original_log_full'], \n",
    "            sequence_of_events_json=sequence_json_str, \n",
    "            context=context_text\n",
    "        )\n",
    "\n",
    "        # 5. Invoke the LLM directly with the prepared prompt\n",
    "        print(\"Sending final prompt to Gemini LLM for solution generation...\")\n",
    "        try:\n",
    "            llm_response = llm.invoke(final_prompt_for_llm)\n",
    "            solution_json_str = llm_response.content.strip()\n",
    "\n",
    "            if solution_json_str.startswith(\"```json\"):\n",
    "                solution_json_str = solution_json_str.lstrip(\"```json\").rstrip(\"```\").strip()\n",
    "            \n",
    "            try:\n",
    "                generated_solution = json.loads(solution_json_str)\n",
    "                print(\"\\n--- GENERATED INCIDENT RESPONSE PLAN (JSON) ---\")\n",
    "                print(json.dumps(generated_solution, indent=2))\n",
    "                print(\"-----------------------------------------------\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"ERROR: LLM response was not valid JSON for entry {entry_to_analyze.get('event_id', 'N/A')}: {e}\")\n",
    "                print(f\"Raw LLM response content (first 500 chars):\\n {solution_json_str[:500]}\") \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to invoke LLM for entry {entry_to_analyze.get('event_id', 'N/A')}: {e}\")\n",
    "            print(\"Please check your API key, internet connection, and Gemini API quotas.\")\n",
    "\n",
    "else:\n",
    "    print(\"No problematic log entries found to analyze. Please ensure Cell 3 loaded some.\")\n",
    "\n",
    "print(\"\\n--- Full LLM Analysis & Solution Generation Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
