{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "# Control flag to use a smaller sample file for index generation\n",
    "# Set to False to process the full 43GB file for byte-offset indexing\n",
    "USE_SAMPLE_PARSED_FILE_FOR_INDEX = False # <--- Set to False for processing the full 43GB file\n",
    "\n",
    "# Paths for data (relative to project root, as notebook is in 'notebooks/' folder)\n",
    "# Input for building the index\n",
    "FULL_PARSED_JSONL_PATH = \"../data/parsed_logs/hdfs_v2_parsed_hybrid_final.jsonl\"\n",
    "SAMPLE_PARSED_JSONL_PATH = \"../data/parsed_logs/sample_parsed_logs.jsonl\" # Path to your manually created sample file\n",
    "\n",
    "# Dynamically set the input path based on the flag\n",
    "if USE_SAMPLE_PARSED_FILE_FOR_INDEX:\n",
    "    INPUT_PARSED_JSONL_FOR_INDEXING = SAMPLE_PARSED_JSONL_PATH\n",
    "else:\n",
    "    INPUT_PARSED_JSONL_FOR_INDEXING = FULL_PARSED_JSONL_PATH\n",
    "\n",
    "# Output for the generated byte-offset index\n",
    "OUTPUT_OFFSET_INDEX_FILE = \"../data/parsed_logs/hdfs_v2_offset_index.csv\" \n",
    "\n",
    "# Ensure the output directory for the index exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_OFFSET_INDEX_FILE), exist_ok=True)\n",
    "\n",
    "# --- Print Statements for Clarity ---\n",
    "print(\"Setup complete. Paths configured for Byte-Offset Indexing.\")\n",
    "print(f\"Using input file for indexing: {INPUT_PARSED_JSONL_FOR_INDEXING}\")\n",
    "if not os.path.exists(INPUT_PARSED_JSONL_FOR_INDEXING):\n",
    "    print(f\"ERROR: Input JSONL file NOT FOUND at {INPUT_PARSED_JSONL_FOR_INDEXING}.\")\n",
    "    if USE_SAMPLE_PARSED_FILE_FOR_INDEX:\n",
    "        print(\"Please ensure 'sample_parsed_logs.jsonl' is created and placed in 'data/parsed_logs/'.\")\n",
    "    else:\n",
    "        print(\"Please ensure 'hdfs_v2_parsed_hybrid_final.jsonl' exists or set USE_SAMPLE_PARSED_FILE_FOR_INDEX=True for testing.\")\n",
    "\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Building byte-offset index for {INPUT_PARSED_JSONL_FOR_INDEXING}...\")\n",
    "\n",
    "offset_map_data = [] # List to store dictionaries for DataFrame\n",
    "current_byte_offset = 0 # Track byte offset in the parsed JSONL file\n",
    "\n",
    "try:\n",
    "    with open(INPUT_PARSED_JSONL_FOR_INDEXING, 'r', encoding='utf-8', errors='ignore') as f_in:\n",
    "        # Use tqdm to show progress for reading the large JSONL file\n",
    "        for i, line_str in tqdm(enumerate(f_in), desc=\"Generating byte offset index\"): # <--- tqdm is applied here\n",
    "            # Store the current line's offset\n",
    "            \n",
    "            # --- CRUCIAL CHANGE: Parse the JSON line to get its metadata including source_file ---\n",
    "            try:\n",
    "                parsed_entry = json.loads(line_str)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Error decoding JSON in offset index build (Line {i+1}): {e}. Skipping line.\")\n",
    "                current_byte_offset += len(line_str.encode('utf-8')) # Still update offset for next line\n",
    "                continue # Skip malformed JSON lines\n",
    "            \n",
    "            source_file_name = parsed_entry.get('source_file')\n",
    "            line_id_in_original_file = parsed_entry.get('line_id_in_file_header')\n",
    "\n",
    "            if source_file_name is None or line_id_in_original_file is None:\n",
    "                print(f\"Warning: Missing 'source_file' or 'line_id_in_file_header' in parsed entry (Line {i+1}). Skipping line for index.\")\n",
    "                current_byte_offset += len(line_str.encode('utf-8'))\n",
    "                continue # Skip if essential metadata is missing\n",
    "\n",
    "            # Store the current line's offset along with its original file and line ID\n",
    "            offset_map_data.append({\n",
    "                'source_file': source_file_name,\n",
    "                'line_id_in_file_header': line_id_in_file_header,\n",
    "                'byte_offset': current_byte_offset\n",
    "            })\n",
    "            \n",
    "            # Update offset for the next line (add length of current line + newline char)\n",
    "            current_byte_offset += len(line_str.encode('utf-8')) # Encode to bytes to get correct byte length\n",
    "\n",
    "    print(f\"\\nSuccessfully created index for {len(offset_map_data)} lines.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Parsed JSONL file not found at {INPUT_PARSED_JSONL_FOR_INDEXING}. Please check path and ensure it's generated.\")\n",
    "    offset_map_data = [] # Clear data to prevent further errors\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during index generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45477644",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving byte-offset index to {OUTPUT_OFFSET_INDEX_FILE}...\")\n",
    "\n",
    "if offset_map_data:\n",
    "    offset_df = pd.DataFrame(offset_map_data)\n",
    "    offset_df.to_csv(OUTPUT_OFFSET_INDEX_FILE, index=False)\n",
    "    print(f\"Byte-offset index saved successfully with {len(offset_df)} entries.\")\n",
    "else:\n",
    "    print(\"WARNING: No offset data to save. Index was not generated.\")\n",
    "\n",
    "print(f\"\\nExample of saved offset index (first 5 rows):\\n{offset_df.head()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
