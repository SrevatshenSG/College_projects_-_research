{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12be9f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini LLM initialized.\n",
      "Gemini Embedding model initialized.\n",
      "\n",
      "Full setup complete. Paths and models configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# LangChain components for RAG\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredFileLoader, UnstructuredMarkdownLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS # Using FAISS for persistent vector store\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv() # Load your API key from .env\n",
    "\n",
    "# Paths for data (relative to project root, as notebook is in 'notebooks/' folder)\n",
    "# Input for sequence retrieval\n",
    "INPUT_PARSED_JSONL_FULL = \"../data/parsed_logs/hdfs_v2_parsed_hybrid_final.jsonl\" # The 43GB parsed log file\n",
    "INPUT_OFFSET_INDEX_CSV = \"../data/parsed_logs/hdfs_v2_offset_index.csv\" # The generated byte-offset index\n",
    "\n",
    "# Input for RAG knowledge base\n",
    "SOLUTION_DOCS_DIR = \"../data/solution_docs/\"\n",
    "\n",
    "# Path where FAISS index will persist its database (relative to project root)\n",
    "FAISS_INDEX_PATH = \"../faiss_index/\" \n",
    "\n",
    "# General parameters\n",
    "NUM_PRECEDING_LOGS_FOR_SEQUENCE = 10 # Number of lines before problematic one to retrieve for sequence\n",
    "PROBLEMATIC_TEMPLATES_LIST_PATH = \"../data/templates/problematic_templates_list.txt\" # For sample problems\n",
    "\n",
    "# --- LLM and Embedding Model Initialization ---\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\", \n",
    "    temperature=0.0, \n",
    "    max_output_tokens=4096, # Increased maximum output tokens\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "print(\"Gemini LLM initialized.\")\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\", \n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "print(\"Gemini Embedding model initialized.\")\n",
    "\n",
    "# Create necessary output directories (relative to project root)\n",
    "os.makedirs(os.path.dirname(INPUT_PARSED_JSONL_FULL), exist_ok=True) # Ensures data/parsed_logs\n",
    "os.makedirs(os.path.dirname(INPUT_OFFSET_INDEX_CSV), exist_ok=True) # Ensures data/parsed_logs\n",
    "os.makedirs(SOLUTION_DOCS_DIR, exist_ok=True) # Ensures data/solution_docs\n",
    "os.makedirs(FAISS_INDEX_PATH, exist_ok=True) # Ensures faiss_index/\n",
    "\n",
    "# Ensure input files for byte index and parsed logs exist\n",
    "if not os.path.exists(INPUT_PARSED_JSONL_FULL):\n",
    "    print(f\"ERROR: Full parsed JSONL file NOT FOUND at {INPUT_PARSED_JSONL_FULL}.\")\n",
    "    print(\"Please ensure you have run '01_hdfs_log_parsing_hybrid.ipynb' and copied the output.\")\n",
    "if not os.path.exists(INPUT_OFFSET_INDEX_CSV):\n",
    "    print(f\"ERROR: Byte-offset index CSV NOT FOUND at {INPUT_OFFSET_INDEX_CSV}.\")\n",
    "    print(f\"Please run '05_byte_offset_indexing.ipynb' first to create it.\")\n",
    "\n",
    "print(\"\\nFull setup complete. Paths and models configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98f8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Byte-Offset Index ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a195a6725b87496ab07497955d113aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading offset map:   0%|          | 0/58095583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded byte-offset index with 58095583 entries.\n",
      "\n",
      "--- Building RAG Knowledge Base ---\n",
      "No existing valid FAISS index found or loading failed. Building from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pypdf\\_crypt_providers\\_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 52 pages/chunks from ITM_Hadoop_troubleshooting_guide.pdf\n",
      "\n",
      "Total raw documents loaded for RAG: 52\n",
      "Splitting documents into smaller chunks...\n",
      "Split into 128 chunks for RAG.\n",
      "Creating embeddings and storing in FAISS (in-memory and saving to disk at ../faiss_index/)...\n",
      "Embeddings created and stored in FAISS, and saved to disk.\n",
      "RAG retriever initialized.\n",
      "\n",
      "--- RAG Knowledge Base Setup Complete ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Loading Byte-Offset Index ---\")\n",
    "offset_map = {} # Initialize empty map\n",
    "\n",
    "if os.path.exists(INPUT_OFFSET_INDEX_CSV):\n",
    "    try:\n",
    "        offset_df = pd.read_csv(INPUT_OFFSET_INDEX_CSV)\n",
    "        for index, row in tqdm(offset_df.iterrows(), total=len(offset_df), desc=\"Loading offset map\"):\n",
    "            offset_map[(str(row['source_file']), int(row['line_id_in_file_header']))] = int(row['byte_offset'])\n",
    "        print(f\"Loaded byte-offset index with {len(offset_map)} entries.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load offset index CSV: {e}\")\n",
    "        print(\"Please ensure '05_byte_offset_indexing.ipynb' ran successfully and generated a valid CSV.\")\n",
    "else:\n",
    "    print(\"WARNING: Byte-offset index CSV not found. Sequence retrieval will not work.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Building RAG Knowledge Base ---\")\n",
    "\n",
    "# Check if FAISS index already exists on disk\n",
    "faiss_index_exists = os.path.exists(FAISS_INDEX_PATH) and os.listdir(FAISS_INDEX_PATH)\n",
    "\n",
    "if faiss_index_exists:\n",
    "    print(f\"Loading existing FAISS index from {FAISS_INDEX_PATH}...\")\n",
    "    try:\n",
    "        vectorstore = FAISS.load_local(FAISS_INDEX_PATH, embedding_model, allow_dangerous_deserialization=True) \n",
    "        print(\"FAISS index loaded successfully and contains data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index: {e}. Rebuilding from scratch.\")\n",
    "        faiss_index_exists = False \n",
    "\n",
    "if not faiss_index_exists: \n",
    "    print(\"No existing valid FAISS index found or loading failed. Building from scratch...\")\n",
    "    \n",
    "    # 1. Load Solution Documents\n",
    "    solution_doc_files_to_load = []\n",
    "    if not os.path.exists(SOLUTION_DOCS_DIR):\n",
    "        print(f\"ERROR: Solution documents directory not found at {SOLUTION_DOCS_DIR}. RAG will not have context.\")\n",
    "    else:\n",
    "        for root, _, files in os.walk(SOLUTION_DOCS_DIR):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file_path.endswith('.pdf'):\n",
    "                    loader = PyPDFLoader(file_path) \n",
    "                elif file_path.endswith(('.txt', '.md', '.html', '.docx', '.xlsx')):\n",
    "                    loader = UnstructuredFileLoader(file_path) # Requires unstructured[all-extra]\n",
    "                else:\n",
    "                    print(f\"Skipping unsupported file type: {file_path}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    docs = loader.load()\n",
    "                    solution_doc_files_to_load.extend(docs)\n",
    "                    print(f\"Loaded {len(docs)} pages/chunks from {os.path.basename(file_path)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Could not load {file_path}: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal raw documents loaded for RAG: {len(solution_doc_files_to_load)}\")\n",
    "    if not solution_doc_files_to_load:\n",
    "        print(\"WARNING: No solution documents were loaded. RAG will not have context.\")\n",
    "\n",
    "    # 2. Split Documents into Chunks\n",
    "    print(\"Splitting documents into smaller chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,     # Size of each chunk\n",
    "        chunk_overlap=200,   # Overlap between chunks to maintain context\n",
    "        length_function=len  # Use character length\n",
    "    )\n",
    "    rag_chunks = text_splitter.split_documents(solution_doc_files_to_load)\n",
    "    print(f\"Split into {len(rag_chunks)} chunks for RAG.\")\n",
    "\n",
    "    if not rag_chunks:\n",
    "        print(\"WARNING: No chunks created for RAG. Check document loading or text splitter settings.\")\n",
    "\n",
    "    # 3. Create Embeddings and Store in FAISS\n",
    "    print(f\"Creating embeddings and storing in FAISS (in-memory and saving to disk at {FAISS_INDEX_PATH})...\")\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=rag_chunks,\n",
    "        embedding=embedding_model\n",
    "    )\n",
    "    vectorstore.save_local(FAISS_INDEX_PATH) # Save FAISS index to disk\n",
    "    print(\"Embeddings created and stored in FAISS, and saved to disk.\")\n",
    "\n",
    "# 4. Create Retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"RAG retriever initialized.\")\n",
    "\n",
    "print(\"\\n--- RAG Knowledge Base Setup Complete ---\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d7c18d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence retrieval function defined.\n",
      "\n",
      "--- Loading Sample Problematic Templates ---\n",
      "Loaded 47 problematic templates for testing.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Sequence Retrieval Function ---\n",
    "def get_contextual_log_sequence_from_disk(\n",
    "    all_parsed_jsonl_path: str, # Path to the 43GB JSONL file\n",
    "    target_entry_metadata: dict, # Problematic entry's metadata: {'source_file': ..., 'line_id_in_file_header': ...}\n",
    "    num_lines_before: int, \n",
    "    offset_map: dict # The loaded (source_file, line_id) -> byte_offset map\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Retrieves a sequence of log entries preceding a target problematic entry from the JSONL file on disk.\n",
    "    Uses the byte-offset map for efficient seeking.\n",
    "    \"\"\"\n",
    "    target_source_file = target_entry_metadata.get('source_file')\n",
    "    target_line_id = target_entry_metadata.get('line_id_in_file_header')\n",
    "\n",
    "    if not target_source_file or not isinstance(target_line_id, int):\n",
    "        print(f\"Error in get_contextual_log_sequence_from_disk: Missing 'source_file' or 'line_id_in_file_header' in target_entry_metadata.\")\n",
    "        return []\n",
    "\n",
    "    sequence = []\n",
    "    \n",
    "    # Calculate the start line ID for the sequence\n",
    "    start_line_id_in_file = max(1, target_line_id - num_lines_before)\n",
    "    \n",
    "    # Attempt to find the byte offset for the start of the sequence\n",
    "    # Offset map key is (source_file, line_id_in_file_header)\n",
    "    start_offset_key = (target_source_file, start_line_id_in_file)\n",
    "    start_byte_offset = offset_map.get(start_offset_key)\n",
    "\n",
    "    if start_byte_offset is None:\n",
    "        # If the exact start_line_id isn't in the map, this means either:\n",
    "        # 1. The very first few lines of the file are being requested (before the map's start).\n",
    "        # 2. The offset_map doesn't contain an entry for this specific line_id.\n",
    "        print(f\"Warning: Exact start offset for line {start_line_id_in_file} in {target_source_file} not found in offset map. Returning empty sequence.\")\n",
    "        return [] \n",
    "\n",
    "    # Read lines from the parsed JSONL file on disk\n",
    "    try:\n",
    "        with open(all_parsed_jsonl_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            f.seek(start_byte_offset) # Jump directly to the approximate start of the sequence\n",
    "\n",
    "            lines_read_from_seek = 0\n",
    "            # Read enough lines to cover the sequence, plus some buffer, until target line_id\n",
    "            for line in f:\n",
    "                if lines_read_from_seek >= num_lines_before + 5: # Read up to 2*N lines plus buffer to be safe\n",
    "                    break \n",
    "\n",
    "                try:\n",
    "                    parsed_seq_entry = json.loads(line)\n",
    "                    # Add only logs from the same source file and BEFORE the target line_id\n",
    "                    if (parsed_seq_entry.get('source_file') == target_source_file and \n",
    "                        parsed_seq_entry.get('line_id_in_file_header', 0) < target_line_id):\n",
    "                        sequence.append(parsed_seq_entry)\n",
    "                    lines_read_from_seek += 1\n",
    "                except json.JSONDecodeError:\n",
    "                    pass \n",
    "\n",
    "        # Ensure the sequence is sorted by line_id (chronological) and limited to num_lines_before\n",
    "        sequence.sort(key=lambda x: x.get('line_id_in_file_header', 0))\n",
    "        sequence = sequence[-num_lines_before:] # Take the last N lines, which are the preceding ones\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving sequence from disk for {target_source_file} line {target_line_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "    return sequence\n",
    "\n",
    "print(\"Sequence retrieval function defined.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Loading Sample Problematic Templates ---\")\n",
    "problematic_templates_for_test = []\n",
    "current_template_data = {}\n",
    "current_section = None\n",
    "\n",
    "def clean_value(line): # Helper function from previous notebook\n",
    "    return line.split(':', 1)[1].strip() if ':' in line else line.strip()\n",
    "\n",
    "if os.path.exists(PROBLEMATIC_TEMPLATES_LIST_PATH):\n",
    "    with open(PROBLEMATIC_TEMPLATES_LIST_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"--- Template ID:\"):\n",
    "                if current_template_data: # Save previous template if exists\n",
    "                    problematic_templates_for_test.append(current_template_data)\n",
    "                \n",
    "                template_id = line.split(\":\", 1)[1].strip().replace(\" ---\", \"\")\n",
    "                current_template_data = {\"event_id\": template_id, \"original_log_full\": \"\", \"metadata\": {}}\n",
    "                current_section = None \n",
    "            elif line.startswith(\"Level:\"):\n",
    "                current_template_data[\"level\"] = clean_value(line)\n",
    "            elif line.startswith(\"Component:\"):\n",
    "                current_template_data[\"component\"] = clean_value(line)\n",
    "            elif line.startswith(\"Template:\"):\n",
    "                current_template_data[\"event_template\"] = clean_value(line)\n",
    "            elif line.startswith(\"Original Message Sample:\"):\n",
    "                current_section = \"Original Message Sample\"\n",
    "                current_template_data[\"original_log_full\"] = []\n",
    "            elif current_section == \"Original Message Sample\":\n",
    "                if line == \"\" or line.startswith(\"--- Template ID:\") or line.startswith(\"--- End of\"):\n",
    "                    current_template_data[\"original_log_full\"] = \"\\n\".join(current_template_data[\"original_log_full\"]).strip()\n",
    "                    current_section = None \n",
    "                    if line.startswith(\"--- Template ID:\"): \n",
    "                        template_id = line.split(\":\", 1)[1].strip().replace(\" ---\", \"\")\n",
    "                        problematic_templates_for_test.append(current_template_data)\n",
    "                        current_template_data = {\"event_id\": template_id, \"original_log_full\": \"\", \"metadata\": {}}\n",
    "                        current_section = None\n",
    "                else:\n",
    "                    current_template_data[\"original_log_full\"].append(line)\n",
    "        if current_template_data: # Add the last template\n",
    "            if isinstance(current_template_data[\"original_log_full\"], list):\n",
    "                current_template_data[\"original_log_full\"] = \"\\n\".join(current_template_data[\"original_log_full\"]).strip()\n",
    "            problematic_templates_for_test.append(current_template_data)\n",
    "\n",
    "    print(f\"Loaded {len(problematic_templates_for_test)} problematic templates for testing.\")\n",
    "else:\n",
    "    print(f\"ERROR: Problematic templates list file NOT FOUND at {PROBLEMATIC_TEMPLATES_LIST_PATH}.\")\n",
    "\n",
    "if not problematic_templates_for_test:\n",
    "    print(\"WARNING: No problematic templates loaded. Sequence retrieval test will be skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b77e4568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining LLM Prompt Template ---\n",
      "LLM Prompt Template defined.\n",
      "\n",
      "--- LLM Analysis & Solution Generation ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69574db0a6ce43dd8ea43574b26d9fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing problematic templates:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Template 1 (ID: HDFS_6C708D31, Level: WARN) ---\n",
      "Event Template: Slow BlockReceiver write packet to mirror took <NUM>ms (threshold=<NUM>ms)\n",
      "Original Full Log Snippet:\n",
      "2016-04-13 21:56:12,682 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 340ms (threshold=300ms)...\n",
      "\n",
      "NOTE: Sequence retrieval in this test uses a MOCKED target entry metadata.\n",
      "For a real sequence, 'sample_entry' needs 'source_file'/'line_id_in_file_header'.\n",
      "Retrieved 10 preceding log entries for sequence analysis.\n",
      "Retrieving relevant context from knowledge base...\n",
      "Retrieved 4 document chunks.\n",
      "Sending final prompt to Gemini LLM for solution generation...\n",
      "\n",
      "--- GENERATED INCIDENT RESPONSE PLAN (JSON) ---\n",
      "{\n",
      "  \"summary\": \"The DataNode is experiencing slow write operations to its mirror DataNode, exceeding the configured threshold of 300ms. The log entry 'Slow BlockReceiver write packet to mirror took 340ms (threshold=300ms)' indicates a performance bottleneck during data replication. The preceding log entries show a large number of blocks being deleted by the FsDatasetAsyncDiskService, which could be contributing to increased disk I/O and network congestion, potentially impacting replication performance.\",\n",
      "  \"severity\": \"Medium\",\n",
      "  \"root_cause_hypothesis\": \"The slow write operations to the mirror DataNode are likely caused by a combination of factors, including increased disk I/O due to concurrent block deletions by the FsDatasetAsyncDiskService, network congestion, or resource contention on either the source or destination DataNode. The block deletions might be triggering increased garbage collection or other background processes that are impacting performance. It's also possible that the mirror DataNode is experiencing hardware issues or is overloaded.\",\n",
      "  \"affected_components\": [\n",
      "    \"DataNode\",\n",
      "    \"BlockReceiver\",\n",
      "    \"FsDatasetAsyncDiskService\",\n",
      "    \"HDFS Replication Pipeline\",\n",
      "    \"Network\"\n",
      "  ],\n",
      "  \"response_plan\": {\n",
      "    \"devops_sre_actions\": [\n",
      "      {\n",
      "        \"step_description\": \"Check DataNode resource utilization (CPU, memory, disk I/O, network I/O) on both the source and mirror DataNodes.\",\n",
      "        \"command\": \"top, iostat, iftop, htop\",\n",
      "        \"type\": \"diagnosis\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Monitor network latency and bandwidth between the source and mirror DataNodes.\",\n",
      "        \"command\": \"ping, traceroute, iperf\",\n",
      "        \"type\": \"diagnosis\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Check HDFS disk space utilization on both DataNodes.\",\n",
      "        \"command\": \"hdfs dfsadmin -report\",\n",
      "        \"type\": \"diagnosis\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Examine DataNode logs for related errors or warnings, focusing on disk I/O and network activity.\",\n",
      "        \"command\": \"grep -i 'error\\\\|warn' hadoop-hdfs-datanode-*.log\",\n",
      "        \"type\": \"diagnosis\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Temporarily reduce the number of concurrent block deletions performed by the FsDatasetAsyncDiskService (if possible via configuration) to see if it alleviates the issue.\",\n",
      "        \"command\": \"hdfs site.xml configuration change (if applicable)\",\n",
      "        \"type\": \"mitigation\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"If network congestion is suspected, investigate network infrastructure and consider increasing network bandwidth or optimizing network configuration.\",\n",
      "        \"command\": \"network monitoring tools, network configuration review\",\n",
      "        \"type\": \"mitigation\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Check the health and status of the disks on both DataNodes. Look for SMART errors or other indications of disk failure.\",\n",
      "        \"command\": \"smartctl -a /dev/sda (replace /dev/sda with the appropriate disk device)\",\n",
      "        \"type\": \"diagnosis\"\n",
      "      }\n",
      "    ],\n",
      "    \"developer_actions\": [\n",
      "      {\n",
      "        \"step_description\": \"Review the FsDatasetAsyncDiskService code to identify potential performance bottlenecks during block deletion.\",\n",
      "        \"command\": \"code review\",\n",
      "        \"type\": \"diagnosis\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Investigate the logic that triggers block deletions to understand why so many blocks are being deleted concurrently.\",\n",
      "        \"command\": \"code review, debugging\",\n",
      "        \"type\": \"diagnosis\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Analyze the impact of block deletions on garbage collection and other background processes.\",\n",
      "        \"command\": \"JVM profiling tools\",\n",
      "        \"type\": \"diagnosis\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Consider implementing rate limiting or throttling for block deletions to reduce the impact on replication performance.\",\n",
      "        \"command\": \"code modification\",\n",
      "        \"type\": \"mitigation\"\n",
      "      }\n",
      "    ],\n",
      "    \"security_actions\": [\n",
      "      {\n",
      "        \"step_description\": \"Verify that the block deletions are authorized and expected.\",\n",
      "        \"command\": \"audit logs, access control lists\",\n",
      "        \"type\": \"investigation\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Ensure that the data being replicated is properly encrypted in transit and at rest.\",\n",
      "        \"command\": \"HDFS encryption configuration review\",\n",
      "        \"type\": \"verification\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Check for any unusual network traffic patterns that might indicate a security breach.\",\n",
      "        \"command\": \"network intrusion detection system (NIDS) logs\",\n",
      "        \"type\": \"investigation\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "-----------------------------------------------\n",
      "\n",
      "--- Analyzing Template 2 (ID: HDFS_CC52BBF3, Level: ERROR) ---\n",
      "Event Template: RECEIVED SIGNAL <NUM>: SIGTERM\n",
      "Original Full Log Snippet:\n",
      "2016-07-26 12:02:27,525 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM...\n",
      "\n",
      "NOTE: Sequence retrieval in this test uses a MOCKED target entry metadata.\n",
      "For a real sequence, 'sample_entry' needs 'source_file'/'line_id_in_file_header'.\n",
      "Retrieved 10 preceding log entries for sequence analysis.\n",
      "Retrieving relevant context from knowledge base...\n",
      "Retrieved 4 document chunks.\n",
      "Sending final prompt to Gemini LLM for solution generation...\n",
      "\n",
      "--- GENERATED INCIDENT RESPONSE PLAN (JSON) ---\n",
      "{\n",
      "  \"summary\": \"The DataNode on host mesos-01 received a SIGTERM signal, indicating a controlled shutdown or termination of the process. The preceding logs show the DataNode actively deleting blocks, suggesting a possible decommissioning or rebalancing operation was in progress before the termination signal was received.\",\n",
      "  \"severity\": \"Medium\",\n",
      "  \"root_cause_hypothesis\": \"The most likely root cause is a planned decommissioning or maintenance operation that triggered a SIGTERM signal to the DataNode process. Another possibility is an external process management system (e.g., Mesos scheduler) terminating the DataNode due to resource constraints or scheduling policies. A less likely, but possible, cause is a manual termination of the DataNode process by an administrator.\",\n",
      "  \"affected_components\": [\n",
      "    \"HDFS DataNode\",\n",
      "    \"HDFS Block Management\",\n",
      "    \"HDFS Replication\"\n",
      "  ],\n",
      "  \"response_plan\": {\n",
      "    \"devops_sre_actions\": [\n",
      "      {\n",
      "        \"step_description\": \"Verify DataNode status in the NameNode UI.\",\n",
      "        \"command\": \"Access the NameNode web UI and check the status of the DataNode. Look for 'Decommissioned' or 'In Maintenance' status.\",\n",
      "        \"type\": \"Investigation\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Check HDFS decommissioning logs on the NameNode.\",\n",
      "        \"command\": \"grep -i 'decommission' namenode.log*\",\n",
      "        \"type\": \"Investigation\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Check Mesos scheduler logs for DataNode termination events.\",\n",
      "        \"command\": \"Access Mesos UI or logs and search for events related to the DataNode container/task termination.\",\n",
      "        \"type\": \"Investigation\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Check system logs for manual termination commands.\",\n",
      "        \"command\": \"grep 'kill -15' /var/log/syslog /var/log/messages /var/log/secure\",\n",
      "        \"type\": \"Investigation\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Restart the DataNode service.\",\n",
      "        \"command\": \"hdfs --daemon start datanode\",\n",
      "        \"type\": \"Remediation\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Monitor DataNode startup and block replication.\",\n",
      "        \"command\": \"tail -f hadoop-hdfs-datanode-mesos-01.log\",\n",
      "        \"type\": \"Monitoring\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"If decommissioning was unintentional, investigate the decommissioning process and prevent future occurrences.\",\n",
      "        \"command\": \"Review decommissioning scripts and automation tools.\",\n",
      "        \"type\": \"Prevention\"\n",
      "      }\n",
      "    ],\n",
      "    \"developer_actions\": [\n",
      "      {\n",
      "        \"step_description\": \"Review DataNode decommissioning code for potential bugs.\",\n",
      "        \"command\": \"N/A - Code review of decommissioning scripts/tools.\",\n",
      "        \"type\": \"Code Review\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Check for any recent changes to DataNode configuration or deployment scripts.\",\n",
      "        \"command\": \"git log -p datanode_config.xml datanode_deployment.sh\",\n",
      "        \"type\": \"Code Review\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Investigate potential race conditions during block deletion and DataNode shutdown.\",\n",
      "        \"command\": \"N/A - Requires in-depth code analysis and potentially debugging.\",\n",
      "        \"type\": \"Debugging\"\n",
      "      }\n",
      "    ],\n",
      "    \"security_actions\": [\n",
      "      {\n",
      "        \"step_description\": \"Verify that the decommissioning process is authorized and audited.\",\n",
      "        \"command\": \"Review audit logs for decommissioning events and ensure proper authorization mechanisms are in place.\",\n",
      "        \"type\": \"Audit\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Ensure that data is properly replicated before decommissioning a DataNode to prevent data loss.\",\n",
      "        \"command\": \"Check HDFS replication factor and ensure sufficient replicas exist before decommissioning.\",\n",
      "        \"type\": \"Verification\"\n",
      "      },\n",
      "      {\n",
      "        \"step_description\": \"Investigate if the SIGTERM signal was sent maliciously.\",\n",
      "        \"command\": \"Analyze system logs for suspicious activity related to the DataNode process.\",\n",
      "        \"type\": \"Investigation\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "-----------------------------------------------\n",
      "\n",
      "--- Analyzing Template 3 (ID: HDFS_9BC8E9E2, Level: WARN) ---\n",
      "Event Template: IOException in offerService\n",
      "Original Full Log Snippet:\n",
      "2016-07-28 15:43:29,170 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService\n",
      "java.io.EOFException: End of File Exception between local host is: \"mesos-master-1/10.10.34.11\"; destination host is: \"mesos-master-1\":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\n",
      "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.jav...\n",
      "\n",
      "NOTE: Sequence retrieval in this test uses a MOCKED target entry metadata.\n",
      "For a real sequence, 'sample_entry' needs 'source_file'/'line_id_in_file_header'.\n",
      "Retrieved 10 preceding log entries for sequence analysis.\n",
      "Retrieving relevant context from knowledge base...\n",
      "Retrieved 4 document chunks.\n",
      "Sending final prompt to Gemini LLM for solution generation...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 124\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSending final prompt to Gemini LLM for solution generation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     llm_response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_prompt_for_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     solution_json_str = llm_response.content.strip()\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m solution_json_str.startswith(\u001b[33m\"\u001b[39m\u001b[33m```json\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:158\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    149\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     **kwargs: Any,\n\u001b[32m    154\u001b[39m ) -> BaseMessage:\n\u001b[32m    155\u001b[39m     config = ensure_config(config)\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    157\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    168\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:560\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    553\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    554\u001b[39m     prompts: List[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    557\u001b[39m     **kwargs: Any,\n\u001b[32m    558\u001b[39m ) -> LLMResult:\n\u001b[32m    559\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:421\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    420\u001b[39m             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    422\u001b[39m flattened_outputs = [\n\u001b[32m    423\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    425\u001b[39m ]\n\u001b[32m    426\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:411\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    410\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m         )\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:632\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    636\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:555\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    544\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    545\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    548\u001b[39m     **kwargs: Any,\n\u001b[32m    549\u001b[39m ) -> ChatResult:\n\u001b[32m    550\u001b[39m     params, chat, message = \u001b[38;5;28mself\u001b[39m._prepare_chat(\n\u001b[32m    551\u001b[39m         messages,\n\u001b[32m    552\u001b[39m         stop=stop,\n\u001b[32m    553\u001b[39m         **kwargs,\n\u001b[32m    554\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m     response: genai.types.GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:152\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:336\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    334\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    335\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:398\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:134\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\generativeai\\generative_models.py:426\u001b[39m, in \u001b[36mChatSession.send_message\u001b[39m\u001b[34m(self, content, generation_config, safety_settings, stream, tools)\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config.get(\u001b[33m\"\u001b[39m\u001b[33mcandidate_count\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt chat with `candidate_count > 1`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28mself\u001b[39m._check_response(response=response, stream=stream)\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enable_automatic_function_calling \u001b[38;5;129;01mand\u001b[39;00m tools_lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\generativeai\\generative_models.py:232\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, request_options)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:566\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    561\u001b[39m metadata = \u001b[38;5;28mtuple\u001b[39m(metadata) + (\n\u001b[32m    562\u001b[39m     gapic_v1.routing_header.to_grpc_metadata(((\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, request.model),)),\n\u001b[32m    563\u001b[39m )\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\grpc\\_channel.py:1178\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1166\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1168\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1173\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1174\u001b[39m ) -> Any:\n\u001b[32m   1175\u001b[39m     (\n\u001b[32m   1176\u001b[39m         state,\n\u001b[32m   1177\u001b[39m         call,\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\srevatshen.sg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\grpc\\_channel.py:1162\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1145\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1146\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1147\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1160\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1161\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1163\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"--- Defining LLM Prompt Template ---\")\n",
    "\n",
    "# Define the Prompt Template for the LLM\n",
    "prompt_template = \"\"\"\n",
    "You are an expert HDFS (Hadoop Distributed File System) Site Reliability Engineer (SRE) and incident responder.\n",
    "Your task is to analyze a given problematic HDFS log entry, understand the underlying problem, and generate a comprehensive incident response plan.\n",
    "\n",
    "Here is the problematic HDFS log entry for analysis:\n",
    "{log_entry_full}\n",
    "\n",
    "Here is the sequence of preceding log events that led up to this problem:\n",
    "{sequence_of_events_json}\n",
    "\n",
    "Here is relevant troubleshooting and solution context from our knowledge base:\n",
    "{context}\n",
    "\n",
    "Based on the HDFS log entry, the sequence of events, and the provided context, please perform the following:\n",
    "\n",
    "1.  **Problem Summary:** Provide a concise, clear summary of the issue, referencing insights from the sequence if possible.\n",
    "2.  **Severity Assessment:** Assign a severity level (Critical, High, Medium, Low) based on the log's impact.\n",
    "3.  **Root Cause Hypothesis:** Suggest the most probable root cause(s), specifically mentioning any causal links identified in the sequence.\n",
    "4.  **Affected Components:** List the HDFS components or services that are most likely affected.\n",
    "5.  **Actionable Response Plan (Role-Specific):**\n",
    "    * **DevOps/SRE Actions:** Detailed, step-by-step commands/checks for immediate diagnosis and mitigation.\n",
    "    * **Developer Actions:** Specific areas to check in code, potential data issues, or configurations.\n",
    "    * **Security Actions (if applicable):** Steps to verify/address potential security implications.\n",
    "\n",
    "Format your entire response as a single JSON object with the following keys:\n",
    "\"summary\": \"...\",\n",
    "\"severity\": \"...\",\n",
    "\"root_cause_hypothesis\": \"...\",\n",
    "\"affected_components\": [\"...\", \"...\"],\n",
    "\"response_plan\": {{\n",
    "    \"devops_sre_actions\": [\n",
    "        {{\"step_description\": \"...\", \"command\": \"...\", \"type\": \"...\"}},\n",
    "        {{\"step_description\": \"...\", \"command\": \"...\", \"type\": \"...\"}}\n",
    "    ],\n",
    "    \"developer_actions\": [\n",
    "        {{\"step_description\": \"...\", \"command\": \"...\", \"type\": \"...\"}},\n",
    "        {{\"step_description\": \"...\", \"command\": \"...\", \"type\": \"...\"}}\n",
    "    ],\n",
    "    \"security_actions\": [\n",
    "        {{\"step_description\": \"...\", \"command\": \"...\", \"type\": \"...\"}},\n",
    "        {{\"step_description\": \"...\", \"command\": \"...\", \"type\": \"...\"}}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "**CRITICAL INSTRUCTION:** Ensure your response is ONLY the complete and valid JSON object. Do NOT include any text before or after the JSON. Do NOT truncate the JSON object. It must be perfectly parsable JSON.\n",
    "\"\"\"\n",
    "\n",
    "# Create the PromptTemplate object\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"log_entry_full\", \"sequence_of_events_json\", \"context\"]\n",
    ")\n",
    "\n",
    "print(\"LLM Prompt Template defined.\")\n",
    "print(\"\\n--- LLM Analysis & Solution Generation ---\")\n",
    "\n",
    "if problematic_templates_for_test: # Iterate through samples loaded in Cell 3\n",
    "    # IMPORTANT: For quick testing, you might want to slice this list (e.g., problematic_templates_for_test[:5])\n",
    "    for i, sample_entry in enumerate(tqdm(problematic_templates_for_test, desc=\"Analyzing problematic templates\")):\n",
    "        print(f\"\\n--- Analyzing Template {i+1} (ID: {sample_entry.get('event_id', 'N/A')}, Level: {sample_entry.get('level', 'N/A')}) ---\")\n",
    "        print(f\"Event Template: {sample_entry.get('event_template', 'N/A')}\")\n",
    "        print(f\"Original Full Log Snippet:\\n{sample_entry.get('original_log_full', '')[:500]}...\") # Show a snippet of the log\n",
    "\n",
    "        # 1. Retrieve Contextual Sequence\n",
    "        # Need to construct target_entry_metadata with source_file and line_id for the function\n",
    "        # This is not directly available in problematic_templates_for_test from list.txt\n",
    "        # For testing, we'll use a placeholder or assume a fixed one for now, as direct sequence\n",
    "        # from problematic_templates_for_test is not readily available from the current format.\n",
    "        \n",
    "        # --- IMPORTANT: If you want to test sequence retrieval effectively here,\n",
    "        # 'sample_entry' must contain 'source_file' and 'line_id_in_file_header'\n",
    "        # which it DOES NOT from 'problematic_templates_list.txt'.\n",
    "        # For a full test of sequence, you'd need problematic entries directly from the 43GB JSONL.\n",
    "        # For this notebook's scope, we'll mock it or provide a warning.\n",
    "\n",
    "        mock_target_entry_metadata = {\n",
    "            \"source_file\": \"hadoop-hdfs-datanode-mesos-01.log\", # Example, please adjust for your data\n",
    "            \"line_id_in_file_header\": 80939 # Example, please adjust for your data\n",
    "        }\n",
    "        print(\"\\nNOTE: Sequence retrieval in this test uses a MOCKED target entry metadata.\")\n",
    "        print(\"For a real sequence, 'sample_entry' needs 'source_file'/'line_id_in_file_header'.\")\n",
    "\n",
    "\n",
    "        log_sequence = get_contextual_log_sequence_from_disk(\n",
    "            all_parsed_jsonl_path=INPUT_PARSED_JSONL_FULL, # From Cell 1\n",
    "            target_entry_metadata=mock_target_entry_metadata, # MOCKED\n",
    "            num_lines_before=NUM_PRECEDING_LOGS_FOR_SEQUENCE,\n",
    "            offset_map=offset_map # From Cell 2\n",
    "        )\n",
    "        sequence_json_str = json.dumps(log_sequence, indent=2)\n",
    "        if not log_sequence:\n",
    "            sequence_json_str = \"No preceding log events retrieved (mocked or actual retrieval failed).\"\n",
    "            print(\"Sequence analysis result: No preceding events retrieved.\")\n",
    "        else:\n",
    "            print(f\"Retrieved {len(log_sequence)} preceding log entries for sequence analysis.\")\n",
    "        \n",
    "        # 1. Prepare query for RAG retriever\n",
    "        retriever_query = f\"HDFS troubleshooting for {sample_entry.get('level', 'N/A')} log: {sample_entry.get('event_template', '')}. Provide solution for original message: {sample_entry.get('original_log_full', '')[:200]}\"\n",
    "        \n",
    "        # 2. Retrieve relevant context from knowledge base\n",
    "        print(\"Retrieving relevant context from knowledge base...\")\n",
    "        retrieved_docs = retriever.invoke(retriever_query)\n",
    "        \n",
    "        context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        if not context_text:\n",
    "            context_text = \"No specific context found in the knowledge base. The LLM will generate a general solution.\"\n",
    "            print(\"WARNING: No specific context retrieved from RAG. Solution may be general.\")\n",
    "        else:\n",
    "            print(f\"Retrieved {len(retrieved_docs)} document chunks.\")\n",
    "\n",
    "        # 3. Prepare the final prompt for the LLM\n",
    "        final_prompt_for_llm = PROMPT.format(\n",
    "            log_entry_full=sample_entry['original_log_full'], \n",
    "            sequence_of_events_json=sequence_json_str, \n",
    "            context=context_text\n",
    "        )\n",
    "\n",
    "        # 4. Invoke the LLM directly with the prepared prompt\n",
    "        print(\"Sending final prompt to Gemini LLM for solution generation...\")\n",
    "        try:\n",
    "            llm_response = llm.invoke(final_prompt_for_llm)\n",
    "            solution_json_str = llm_response.content.strip()\n",
    "\n",
    "            if solution_json_str.startswith(\"```json\"):\n",
    "                solution_json_str = solution_json_str.lstrip(\"```json\").rstrip(\"```\").strip()\n",
    "            \n",
    "            try:\n",
    "                generated_solution = json.loads(solution_json_str)\n",
    "                print(\"\\n--- GENERATED INCIDENT RESPONSE PLAN (JSON) ---\")\n",
    "                print(json.dumps(generated_solution, indent=2))\n",
    "                print(\"-----------------------------------------------\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"ERROR: LLM response was not valid JSON for template {sample_entry.get('event_id', 'N/A')}: {e}\")\n",
    "                print(f\"Raw LLM response content (first 500 chars):\\n {solution_json_str[:500]}\") \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to invoke LLM for template {sample_entry.get('event_id', 'N/A')}: {e}\")\n",
    "            print(\"Please check your API key, internet connection, and Gemini API quotas.\")\n",
    "\n",
    "else:\n",
    "    print(\"No problematic templates found to analyze. Please ensure Cell 3 loaded some.\")\n",
    "\n",
    "print(\"\\n--- LLM Analysis & Solution Generation Complete for Samples ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317570e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining LLM Prompt Template ---\n",
      "LLM Prompt Template defined.\n",
      "\n",
      "--- LLM Analysis & Solution Generation ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c2b162d15f4d3d84ab6fdc5376a881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing problematic entries:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Entry 1 (ID: HDFS_6C708D31, Level: WARN) ---\n",
      "Original Full Log Snippet:\n",
      "2016-04-13 21:56:12,682 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 340ms (threshold=300ms)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'OUTPUT_PARSED_JSONL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal Full Log Snippet:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mentry_to_analyze.get(\u001b[33m'\u001b[39m\u001b[33moriginal_log_full\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)[:\u001b[32m500\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# 1. Retrieve Contextual Sequence\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# This function is defined in Cell 6, which is now executing before this cell.\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# It relies on 'parsed_hdfs_logs_full' (from Cell 6) and 'offset_map' (from Cell 2)\u001b[39;00m\n\u001b[32m     63\u001b[39m log_sequence = get_contextual_log_sequence_from_disk(\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     all_parsed_jsonl_path=\u001b[43mOUTPUT_PARSED_JSONL\u001b[49m, \n\u001b[32m     65\u001b[39m     target_entry_metadata=entry_to_analyze, \n\u001b[32m     66\u001b[39m     num_lines_before=NUM_PRECEDING_LOGS_FOR_SEQUENCE,\n\u001b[32m     67\u001b[39m     offset_map=offset_map\n\u001b[32m     68\u001b[39m )\n\u001b[32m     69\u001b[39m sequence_json_str = json.dumps(log_sequence, indent=\u001b[32m2\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m log_sequence:\n",
      "\u001b[31mNameError\u001b[39m: name 'OUTPUT_PARSED_JSONL' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"--- Defining LLM Prompt Template ---\")\n",
    "\n",
    "# Define the Prompt Template for the LLM\n",
    "prompt_template = \"\"\"\n",
    "You are an expert HDFS (Hadoop Distributed File System) Site Reliability Engineer (SRE) and incident responder.\n",
    "Your task is to concisely analyze a given problematic HDFS log entry, understand the specific underlying problem, and generate a brief, actionable incident response plan.\n",
    "\n",
    "Here is the problematic HDFS log entry for analysis:\n",
    "{log_entry_full}\n",
    "\n",
    "Here is the sequence of preceding log events that led up to this problem:\n",
    "{sequence_of_events_json}\n",
    "\n",
    "Here is relevant troubleshooting and solution context from our knowledge base:\n",
    "{context}\n",
    "\n",
    "Based on the HDFS log entry, the sequence of events, and the provided context, please perform the following:\n",
    "\n",
    "1.  **Problem Summary:** Provide a concise, clear summary of what specifically went wrong, referencing insights from the sequence if possible.\n",
    "2.  **Severity Assessment:** Assign a severity level (Critical, High, Medium, Low) based on the log's impact.\n",
    "3.  **Root Cause Hypothesis:** Suggest the most probable root cause(s). **Explain the flow or chain of events that likely led to this problem, explicitly referencing specific events or patterns from the provided sequence.**\n",
    "4.  **Affected Components:** List the HDFS components or services that are most likely affected.\n",
    "5.  **Actionable Response Plan (Role-Specific):**\n",
    "    * **DevOps/SRE Actions:** Detailed, step-by-step actions.\n",
    "    * **Developer Actions:** Specific areas to check in code, potential data issues, or configurations.\n",
    "    * **Security Actions (if applicable):** Steps to verify/address potential security implications.\n",
    "\n",
    "Format your entire response as a single JSON object with the following keys:\n",
    "\"summary\": \"...\",\n",
    "\"severity\": \"...\",\n",
    "\"root_cause_hypothesis\": \"...\",\n",
    "\"affected_components\": [\"...\", \"...\"],\n",
    "\"response_plan\": {{\n",
    "    \"devops_sre_actions\": [\"...\", \"...\"],\n",
    "    \"developer_actions\": [\"...\", \"...\"],\n",
    "    \"security_actions\": [\"...\", \"...\"]\n",
    "}}\n",
    "\n",
    "**CRITICAL INSTRUCTION:** Ensure your response is ONLY the complete and valid JSON object. Do NOT include any text before or after the JSON. Do NOT truncate the JSON object. It must be perfectly parsable JSON.\n",
    "\"\"\"\n",
    "\n",
    "# Create the PromptTemplate object\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"log_entry_full\", \"sequence_of_events_json\", \"context\"]\n",
    ")\n",
    "\n",
    "print(\"LLM Prompt Template defined.\")\n",
    "print(\"\\n--- LLM Analysis & Solution Generation ---\")\n",
    "\n",
    "if problematic_templates_for_test:\n",
    "    # Iterate through each problematic log entry found in the full dataset\n",
    "    # IMPORTANT: For full runs, this loop can take a very long time and consume many API tokens.\n",
    "    # For initial testing, you might want to slice this list (e.g., problematic_log_entries_full[:5])\n",
    "    for i, entry_to_analyze in enumerate(tqdm(problematic_templates_for_test, desc=\"Analyzing problematic entries\")):\n",
    "        print(f\"\\n--- Analyzing Entry {i+1} (ID: {entry_to_analyze.get('event_id', 'N/A')}, Level: {entry_to_analyze.get('level', 'N/A')}) ---\")\n",
    "        print(f\"Original Full Log Snippet:\\n{entry_to_analyze.get('original_log_full', '')[:500]}...\") \n",
    "\n",
    "        # 1. Retrieve Contextual Sequence\n",
    "        # This function is defined in Cell 6, which is now executing before this cell.\n",
    "        # It relies on 'parsed_hdfs_logs_full' (from Cell 6) and 'offset_map' (from Cell 2)\n",
    "        \n",
    "        log_sequence = get_contextual_log_sequence_from_disk(\n",
    "            all_parsed_jsonl_path=INPUT_PARSED_JSONL_FULL, \n",
    "            target_entry_metadata=entry_to_analyze, \n",
    "            num_lines_before=NUM_PRECEDING_LOGS_FOR_SEQUENCE,\n",
    "            offset_map=offset_map\n",
    "        )\n",
    "        sequence_json_str = json.dumps(log_sequence, indent=2)\n",
    "        if not log_sequence:\n",
    "            sequence_json_str = \"No preceding log events retrieved or sequence unavailable.\"\n",
    "            print(\"Sequence analysis result: No preceding events retrieved.\")\n",
    "        else:\n",
    "            print(f\"Retrieved {len(log_sequence)} preceding log entries for sequence analysis.\")\n",
    "        \n",
    "        # 2. Prepare query for RAG retriever\n",
    "        retriever_query = f\"HDFS troubleshooting for {entry_to_analyze.get('level', 'N/A')} log: {entry_to_analyze.get('event_template', '')}. Provide solution for original message: {entry_to_analyze.get('original_log_full', '')[:200]}\"\n",
    "        \n",
    "        # 3. Retrieve relevant context from knowledge base\n",
    "        print(\"Retrieving relevant context from knowledge base...\")\n",
    "        retrieved_docs = retriever.invoke(retriever_query)\n",
    "        \n",
    "        context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        if not context_text:\n",
    "            context_text = \"No specific context found in the knowledge base. The LLM will generate a general solution.\"\n",
    "            print(\"WARNING: No specific context retrieved from RAG. Solution may be general.\")\n",
    "        else:\n",
    "            print(f\"Retrieved {len(retrieved_docs)} document chunks.\")\n",
    "\n",
    "        # 4. Prepare the final prompt for the LLM\n",
    "        final_prompt_for_llm = PROMPT.format(\n",
    "            log_entry_full=entry_to_analyze['original_log_full'], \n",
    "            sequence_of_events_json=sequence_json_str, \n",
    "            context=context_text\n",
    "        )\n",
    "\n",
    "        # 5. Invoke the LLM directly with the prepared prompt\n",
    "        print(\"Sending final prompt to Gemini LLM for solution generation...\")\n",
    "        try:\n",
    "            llm_response = llm.invoke(final_prompt_for_llm)\n",
    "            solution_json_str = llm_response.content.strip()\n",
    "\n",
    "            if solution_json_str.startswith(\"```json\"):\n",
    "                solution_json_str = solution_json_str.lstrip(\"```json\").rstrip(\"```\").strip()\n",
    "            \n",
    "            try:\n",
    "                generated_solution = json.loads(solution_json_str)\n",
    "                print(\"\\n--- GENERATED INCIDENT RESPONSE PLAN (JSON) ---\")\n",
    "                print(json.dumps(generated_solution, indent=2))\n",
    "                print(\"-----------------------------------------------\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"ERROR: LLM response was not valid JSON for entry {entry_to_analyze.get('event_id', 'N/A')}: {e}\")\n",
    "                print(f\"Raw LLM response content (first 500 chars):\\n {solution_json_str[:500]}\") \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to invoke LLM for entry {entry_to_analyze.get('event_id', 'N/A')}: {e}\")\n",
    "            print(\"Please check your API key, internet connection, and Gemini API quotas.\")\n",
    "\n",
    "else:\n",
    "    print(\"No problematic log entries found to analyze. Please ensure Cell 6 loaded some.\")\n",
    "\n",
    "print(\"\\n--- Full LLM Analysis & Solution Generation Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
