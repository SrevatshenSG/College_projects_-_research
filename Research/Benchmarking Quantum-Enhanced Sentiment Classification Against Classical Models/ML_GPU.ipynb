{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2678a9d0-9c66-403e-8350-33755ef30ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "369a2456-7c4c-43f7-94fc-7553c60a95b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size: 1000\n",
      "Validation Dataset Size: 200\n",
      "\n",
      "Sample Keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Input IDs: tensor([  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026])\n",
      "Label: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Use smaller dataset for faster training/testing (optional)\n",
    "def shrink_dataset(data, size=1000):\n",
    "    return data.select(range(size))\n",
    "\n",
    "dataset[\"train\"] = shrink_dataset(dataset[\"train\"], size=1000)\n",
    "dataset[\"test\"] = shrink_dataset(dataset[\"test\"], size=200)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Custom dataset wrapper\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Tokenize and prepare DataLoaders\n",
    "def prepare_data(tokenizer, dataset_split, max_len=512, batch_size=8):\n",
    "    texts = dataset_split[\"text\"]\n",
    "    labels = dataset_split[\"label\"]\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_len, return_tensors='pt')\n",
    "    imdb_dataset = IMDbDataset(encodings, labels)\n",
    "    dataloader = DataLoader(imdb_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader, imdb_dataset\n",
    "\n",
    "train_loader, train_dataset = prepare_data(tokenizer, dataset[\"train\"])\n",
    "val_loader, val_dataset = prepare_data(tokenizer, dataset[\"test\"])\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
    "print(f\"Validation Dataset Size: {len(val_dataset)}\")\n",
    "\n",
    "# Peek at one sample\n",
    "sample = train_dataset[0]\n",
    "print(\"\\nSample Keys:\", sample.keys())\n",
    "print(\"Input IDs:\", sample['input_ids'][:10])  # Just first 10 tokens\n",
    "print(\"Label:\", sample['labels'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429812bf-2c13-4e7b-b24b-80c77a60e41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Loaded Successfully.\n",
      "Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model for binary classification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Confirm model loaded and check classifier layer\n",
    "print(\"\\nModel Loaded Successfully.\")\n",
    "print(model.classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a86173b-6674-4d45-8095-48d8db661511",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing: {'text': 'I love machine learning', 'label': 1}\n",
      "Tokenizing: {'text': 'This is a great tutorial', 'label': 0}\n",
      "Tokenizing: {'text': 'BERT is an amazing model', 'label': 1}\n",
      "Sample Input IDs: tensor([ 101, 1045, 2293, 3698, 4083,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "Sample Attention Mask: tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Sample Label: 1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Ensure you have your tokenizer loaded (use a specific model)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize the text data\n",
    "def tokenize_data(dataset, tokenizer, max_length=512):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for item in dataset:\n",
    "        # Debugging: print dataset entry to ensure proper structure\n",
    "        print(f\"Tokenizing: {item}\")\n",
    "        \n",
    "        # Tokenize each text in the dataset\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            item['text'],                      # The text from the dataset\n",
    "            truncation=True,                    # Truncate longer sequences\n",
    "            padding='max_length',               # Pad sequences to max_length\n",
    "            max_length=max_length,             # Limit the length to max_length\n",
    "            add_special_tokens=True,           # Add special tokens [CLS] and [SEP]\n",
    "            return_attention_mask=True,        # Return attention mask\n",
    "            return_tensors='pt'                # Return as PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        # Ensure encoding returns tensors\n",
    "        input_ids.append(encoding['input_ids'].squeeze(0))        # Remove batch dimension\n",
    "        attention_masks.append(encoding['attention_mask'].squeeze(0))\n",
    "        labels.append(item['label'])                               # Append label\n",
    "    \n",
    "    # Convert to tensors\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    attention_masks = torch.stack(attention_masks)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "# Example dataset structure for debugging\n",
    "train_dataset = [\n",
    "    {\"text\": \"I love machine learning\", \"label\": 1},\n",
    "    {\"text\": \"This is a great tutorial\", \"label\": 0},\n",
    "    {\"text\": \"BERT is an amazing model\", \"label\": 1}\n",
    "]\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "input_ids, attention_masks, labels = tokenize_data(train_dataset, tokenizer)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_data = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
    "\n",
    "# Sample output check\n",
    "print(f\"Sample Input IDs: {input_ids[0]}\")\n",
    "print(f\"Sample Attention Mask: {attention_masks[0]}\")\n",
    "print(f\"Sample Label: {labels[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5bf8e-77c1-41d8-bacf-5573309ea591",
   "metadata": {},
   "source": [
    "TRAINING CODE (with datset loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8067ef33-4b45-4bd6-a3d3-8307e4ca505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|█████████████████████████████████████████████████████████████████████| 200/200 [16:29<00:00,  4.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.6770 - Accuracy: 55.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|█████████████████████████████████████████████████████████████████████| 200/200 [16:35<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.3567 - Accuracy: 84.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|█████████████████████████████████████████████████████████████████████| 200/200 [16:00<00:00,  4.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.1388 - Accuracy: 95.25%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('saved_model/tokenizer_config.json',\n",
       " 'saved_model/special_tokens_map.json',\n",
       " 'saved_model/vocab.txt',\n",
       " 'saved_model/added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\srevatshen\\imdb-emotion-classifier\\data\\imdb_reviews.csv\")\n",
    "\n",
    "df = df[['review', 'sentiment']].dropna()\n",
    "\n",
    "# Encode labels: positive -> 1, negative -> 0\n",
    "label_encoder = LabelEncoder()\n",
    "df['sentiment'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# Train-test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['review'].tolist(), df['sentiment'].tolist(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encodings = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Dataloaders\n",
    "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = SentimentDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = correct / total * 100\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f} - Accuracy: {acc:.2f}%\")\n",
    "\n",
    "# Save model + tokenizer\n",
    "model.save_pretrained(\"saved_model/\")\n",
    "tokenizer.save_pretrained(\"saved_model/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47fdcbe-32bc-46af-a3c8-bfe2bca5c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a9378-9925-4368-9ab4-acbd9d37654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\n Validation Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\n Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Load model if needed\n",
    "# model = BertForSequenceClassification.from_pretrained(\"saved_model/\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"saved_model/\")\n",
    "# model.to(device)\n",
    "\n",
    "# Evaluate\n",
    "evaluate(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c227619b-b77e-4bee-a82e-3a5f2a9e0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAMLIT LIBRARY FOR FRONT END IMPLEMENTATION OF THE SENTIMENT ANALYSIS MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611bbf45-c596-460c-8e5d-29ea8cf2dcac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from streamlit) (2.0.2)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from streamlit) (11.1.0)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from streamlit) (2.32.3)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from streamlit) (4.13.0)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from streamlit) (3.1.44)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.35.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\srevatshen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/9.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/9.8 MB 3.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.8/9.8 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.6/9.8 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.1/9.8 MB 3.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.7/9.8 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.9/9.8 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.2/9.8 MB 2.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.2/9.8 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.7/9.8 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.7/9.8 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.7/9.8 MB 2.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.0/9.8 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.0/9.8 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 5.8/9.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 6.3/9.8 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.1/9.8 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 7.9/9.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.8 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.8/9.8 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 731.2/731.2 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/6.9 MB 6.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.0/6.9 MB 5.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.0/6.9 MB 5.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.0/6.9 MB 5.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.0/6.9 MB 5.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.0/6.9 MB 5.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.6/6.9 MB 987.0 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 1.6/6.9 MB 987.0 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 1.6/6.9 MB 987.0 kB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 1.8/6.9 MB 868.0 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 2.1/6.9 MB 851.1 kB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 2.9/6.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 4.2/6.9 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.5/6.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/6.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Downloading narwhals-1.35.0-py3-none-any.whl (325 kB)\n",
      "Installing collected packages: watchdog, toml, tenacity, protobuf, narwhals, blinker, pydeck, altair, streamlit\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 narwhals-1.35.0 protobuf-5.29.4 pydeck-0.9.1 streamlit-1.44.1 tenacity-9.1.2 toml-0.10.2 watchdog-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec061b3-1107-445a-a392-f098a83763eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (imdb-env)",
   "language": "python",
   "name": "imdb-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
